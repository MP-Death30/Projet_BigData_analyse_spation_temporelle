{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "494b1670-458c-4574-bb7d-7922f7fa98c9",
   "metadata": {},
   "source": [
    "Import des librairies n√©cessaires √† la pr√©paration des donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73c205b-4860-4cfc-805a-d5f530d9c55a",
   "metadata": {},
   "source": [
    "pip install fsspec[hdfs] pandas geopandas shapely pyproj geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b9f4fa0-c8c6-44d4-8814-92f0179c026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ef04c-ce57-466a-88c0-7ac8fb7168cb",
   "metadata": {},
   "source": [
    "R√©cup√©ration du fichier avec la correspondance id_station et coordonn√©es GPS pour ne garder que les fichiers des stations pr√®s de NYC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225f0e44-3643-436f-9b70-d3f01791295c",
   "metadata": {},
   "source": [
    "Phase 1 : Initialisation de l'Environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d6045c9-0264-4370-95a1-2268f3da9fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session Spark initialis√©e avec les configurations de robustesse.\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "# --- 1. Configuration de Robustesse et Initialisation Spark ---\n",
    "# Augmentation des timeouts et allocation de m√©moire stricte pour √©viter les crashs JVM\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataLake_NOAA_NYC_Prep\") \\\n",
    "    .config(\"spark.executor.memory\", \"3g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.rpc.askTimeout\", \"800s\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# --- 2. D√©finition des Param√®tres G√©ographiques et HDFS ---\n",
    "# Bo√Æte englobante de la r√©gion de NYC\n",
    "MIN_LAT, MAX_LAT = 40.0, 41.5\n",
    "MIN_LON, MAX_LON = -75.0, -73.0\n",
    "\n",
    "# Chemin HDFS BRUT (Corrig√© avec l'h√¥te et le port du namenode)\n",
    "RAW_OUTPUT_PATH = \"hdfs://namenode:9000/user/mathis/datalake/noaa_gsod_nyc_raw_2005_2023.parquet\"\n",
    "\n",
    "# Plage d'ann√©es\n",
    "START_YEAR = 2005\n",
    "END_YEAR = 2023\n",
    "\n",
    "print(\"Session Spark initialis√©e avec les configurations de robustesse.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d29e526-c998-400d-9d18-45236d8b465c",
   "metadata": {},
   "source": [
    "Phase 2 : M√©tadonn√©es et Identification des Stations NOAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a6ad18d-ae5c-4807-95eb-3b54176a858a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ 93 Stations NOAA pertinentes trouv√©es pr√®s de New York.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. T√©l√©chargement des M√©tadonn√©es des Stations (via Pandas car petit fichier) ---\n",
    "stations_url = \"https://www.ncei.noaa.gov/pub/data/noaa/isd-history.csv\"\n",
    "pdf_stations = pd.read_csv(stations_url,\n",
    "                         dtype={'USAF': str, 'WBAN': str})\n",
    "\n",
    "pdf_stations['STN_ID'] = pdf_stations['USAF'].str.strip() + pdf_stations['WBAN'].str.strip()\n",
    "pdf_stations = pdf_stations.rename(columns={'LAT': 'LATITUDE', 'LON': 'LONGITUDE'})\n",
    "pdf_stations = pdf_stations.dropna(subset=['LATITUDE', 'LONGITUDE', 'STATION NAME'])\n",
    "spark_stations_df = spark.createDataFrame(pdf_stations)\n",
    "\n",
    "# --- 2. Filtrage G√©ographique ---\n",
    "nyc_stations_spark = spark_stations_df.filter(\n",
    "    (F.col('LATITUDE') >= MIN_LAT) & (F.col('LATITUDE') <= MAX_LAT) &\n",
    "    (F.col('LONGITUDE') >= MIN_LON) & (F.col('LONGITUDE') <= MAX_LON)\n",
    ")\n",
    "\n",
    "# R√©cup√©ration de la liste des IDs pertinents (pour filtrage par nom de fichier)\n",
    "relevant_station_ids = [row.STN_ID for row in nyc_stations_spark.select(\"STN_ID\").collect()]\n",
    "\n",
    "print(f\"\\n‚úÖ {nyc_stations_spark.count()} Stations NOAA pertinentes trouv√©es pr√®s de New York.\")\n",
    "# Gardons ce DataFrame pour la jointure des coordonn√©es plus tard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21de6ad-c870-438e-bd32-0eb6f005656f",
   "metadata": {},
   "source": [
    "Phase 2 bis : T√©l√©chargement des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbd338ec-5fac-406b-b377-aed46fa0e614",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Prior attempt to load libhdfs failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m END_YEAR \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2023\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Connexion HDFS via fsspec\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m fs \u001b[38;5;241m=\u001b[39m \u001b[43mfsspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhdfs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnamenode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# V√©rification de la liste des stations\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelevant_station_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/fsspec/registry.py:289\u001b[0m, in \u001b[0;36mfilesystem\u001b[0;34m(protocol, **storage_options)\u001b[0m\n\u001b[1;32m    282\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marrow_hdfs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m protocol has been deprecated and will be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mremoved in the future. Specify it as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhdfs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m    286\u001b[0m     )\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m get_filesystem_class(protocol)\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/fsspec/spec.py:79\u001b[0m, in \u001b[0;36m_Cached.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_cache[token]\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# Setting _fs_token here causes some static linters to complain.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_fs_token_ \u001b[38;5;241m=\u001b[39m token\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/fsspec/implementations/arrow.py:278\u001b[0m, in \u001b[0;36mHadoopFileSystem.__init__\u001b[0;34m(self, host, port, user, kerb_ticket, extra_conf, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    Passed on to HadoopFileSystem\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HadoopFileSystem\n\u001b[0;32m--> 278\u001b[0m fs \u001b[38;5;241m=\u001b[39m \u001b[43mHadoopFileSystem\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkerb_ticket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkerb_ticket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_conf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_conf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(fs\u001b[38;5;241m=\u001b[39mfs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyarrow/_hdfs.pyx:95\u001b[0m, in \u001b[0;36mpyarrow._hdfs.HadoopFileSystem.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyarrow/error.pxi:144\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyarrow/error.pxi:115\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Prior attempt to load libhdfs failed"
     ]
    }
   ],
   "source": [
    "import fsspec\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Param√®tres de configuration ---\n",
    "BASE_URL = \"https://www.ncei.noaa.gov/data/global-summary-of-the-day/access\"\n",
    "HDFS_BASE_DIR = \"/user/mathis/raw/noaa_gsod\"  # dossier cible HDFS\n",
    "START_YEAR = 2005\n",
    "END_YEAR = 2023\n",
    "\n",
    "# Connexion HDFS via fsspec\n",
    "fs = fsspec.filesystem(\"hdfs\", host=\"namenode\", port=9000)\n",
    "\n",
    "# V√©rification de la liste des stations\n",
    "if \"relevant_station_ids\" not in locals():\n",
    "    print(\"‚ö†Ô∏è ATTENTION: La liste 'relevant_station_ids' n'est pas d√©finie.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"D√©marrage du t√©l√©chargement NOAA GSOD dans HDFS pour {len(relevant_station_ids)} stations.\")\n",
    "\n",
    "total_files = len(relevant_station_ids) * (END_YEAR - START_YEAR + 1)\n",
    "\n",
    "# --- Barre de progression ---\n",
    "with tqdm(total=total_files, desc=\"T√©l√©chargement GSOD vers HDFS\") as pbar:\n",
    "\n",
    "    for year in range(START_YEAR, END_YEAR + 1):\n",
    "        year_dir = f\"{HDFS_BASE_DIR}/{year}\"\n",
    "\n",
    "        # Cr√©e le dossier HDFS si n√©cessaire\n",
    "        if not fs.exists(year_dir):\n",
    "            fs.mkdir(year_dir)\n",
    "\n",
    "        for station_id in relevant_station_ids:\n",
    "\n",
    "            file_name = f\"{station_id}.csv\"\n",
    "            hdfs_path = f\"{year_dir}/{file_name}\"\n",
    "            remote_url = f\"{BASE_URL}/{year}/{file_name}\"\n",
    "\n",
    "            # V√©rifie si le fichier existe d√©j√† dans HDFS\n",
    "            if fs.exists(hdfs_path):\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # T√©l√©chargement HTTP\n",
    "                response = requests.get(remote_url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                # √âcriture dans HDFS\n",
    "                with fs.open(hdfs_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "\n",
    "                pbar.update(1)\n",
    "                time.sleep(0.1)\n",
    "\n",
    "            except requests.exceptions.HTTPError as errh:\n",
    "                if response.status_code == 404:\n",
    "                    # Station sans donn√©es cette ann√©e ‚Üí normal\n",
    "                    pass\n",
    "                else:\n",
    "                    print(f\"\\n‚ùå Erreur HTTP pour {remote_url}: {errh}\")\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"\\n‚ùå Erreur Connexion/Timeout pour {remote_url}: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ T√©l√©chargement termin√© ‚Äî fichiers GSOD stock√©s dans HDFS.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0b6f7f9-a9c8-49c8-9050-aaf7295e476c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier local cibl√© : /home/jovyan/work/data/noaa_gsod/2005/72224799999.csv\n",
      "\n",
      "--- HEAD du fichier 72224799999.csv ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATION</th>\n",
       "      <th>DATE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>NAME</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>TEMP_ATTRIBUTES</th>\n",
       "      <th>DEWP</th>\n",
       "      <th>DEWP_ATTRIBUTES</th>\n",
       "      <th>...</th>\n",
       "      <th>MXSPD</th>\n",
       "      <th>GUST</th>\n",
       "      <th>MAX</th>\n",
       "      <th>MAX_ATTRIBUTES</th>\n",
       "      <th>MIN</th>\n",
       "      <th>MIN_ATTRIBUTES</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>PRCP_ATTRIBUTES</th>\n",
       "      <th>SNDP</th>\n",
       "      <th>FRSHTT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72224799999</td>\n",
       "      <td>2005-02-18</td>\n",
       "      <td>40.633</td>\n",
       "      <td>-74.667</td>\n",
       "      <td>32.0</td>\n",
       "      <td>SOMERSET, NJ US</td>\n",
       "      <td>27.6</td>\n",
       "      <td>24</td>\n",
       "      <td>12.6</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>16.9</td>\n",
       "      <td>23.9</td>\n",
       "      <td>39.9</td>\n",
       "      <td></td>\n",
       "      <td>24.1</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td>H</td>\n",
       "      <td>999.9</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72224799999</td>\n",
       "      <td>2005-02-19</td>\n",
       "      <td>40.633</td>\n",
       "      <td>-74.667</td>\n",
       "      <td>32.0</td>\n",
       "      <td>SOMERSET, NJ US</td>\n",
       "      <td>23.0</td>\n",
       "      <td>24</td>\n",
       "      <td>3.9</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td></td>\n",
       "      <td>15.1</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td>I</td>\n",
       "      <td>999.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72224799999</td>\n",
       "      <td>2005-02-20</td>\n",
       "      <td>40.633</td>\n",
       "      <td>-74.667</td>\n",
       "      <td>32.0</td>\n",
       "      <td>SOMERSET, NJ US</td>\n",
       "      <td>27.3</td>\n",
       "      <td>24</td>\n",
       "      <td>13.4</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>999.9</td>\n",
       "      <td>37.9</td>\n",
       "      <td></td>\n",
       "      <td>15.1</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td>I</td>\n",
       "      <td>999.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72224799999</td>\n",
       "      <td>2005-02-21</td>\n",
       "      <td>40.633</td>\n",
       "      <td>-74.667</td>\n",
       "      <td>32.0</td>\n",
       "      <td>SOMERSET, NJ US</td>\n",
       "      <td>32.4</td>\n",
       "      <td>24</td>\n",
       "      <td>29.4</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>9.9</td>\n",
       "      <td>16.9</td>\n",
       "      <td>37.9</td>\n",
       "      <td></td>\n",
       "      <td>17.1</td>\n",
       "      <td></td>\n",
       "      <td>0.41</td>\n",
       "      <td>D</td>\n",
       "      <td>999.9</td>\n",
       "      <td>101000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72224799999</td>\n",
       "      <td>2005-02-22</td>\n",
       "      <td>40.633</td>\n",
       "      <td>-74.667</td>\n",
       "      <td>32.0</td>\n",
       "      <td>SOMERSET, NJ US</td>\n",
       "      <td>36.6</td>\n",
       "      <td>24</td>\n",
       "      <td>26.6</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>44.6</td>\n",
       "      <td>*</td>\n",
       "      <td>32.0</td>\n",
       "      <td>*</td>\n",
       "      <td>0.09</td>\n",
       "      <td>B</td>\n",
       "      <td>999.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       STATION        DATE  LATITUDE  LONGITUDE  ELEVATION             NAME  \\\n",
       "0  72224799999  2005-02-18    40.633    -74.667       32.0  SOMERSET, NJ US   \n",
       "1  72224799999  2005-02-19    40.633    -74.667       32.0  SOMERSET, NJ US   \n",
       "2  72224799999  2005-02-20    40.633    -74.667       32.0  SOMERSET, NJ US   \n",
       "3  72224799999  2005-02-21    40.633    -74.667       32.0  SOMERSET, NJ US   \n",
       "4  72224799999  2005-02-22    40.633    -74.667       32.0  SOMERSET, NJ US   \n",
       "\n",
       "   TEMP  TEMP_ATTRIBUTES  DEWP  DEWP_ATTRIBUTES  ...  MXSPD   GUST   MAX  \\\n",
       "0  27.6               24  12.6               24  ...   16.9   23.9  39.9   \n",
       "1  23.0               24   3.9               24  ...   13.0   20.0  32.0   \n",
       "2  27.3               24  13.4               24  ...    7.0  999.9  37.9   \n",
       "3  32.4               24  29.4               24  ...    9.9   16.9  37.9   \n",
       "4  36.6               24  26.6               24  ...   14.0   19.0  44.6   \n",
       "\n",
       "   MAX_ATTRIBUTES   MIN  MIN_ATTRIBUTES  PRCP  PRCP_ATTRIBUTES   SNDP  FRSHTT  \n",
       "0                  24.1                  0.00                H  999.9    1000  \n",
       "1                  15.1                  0.00                I  999.9       0  \n",
       "2                  15.1                  0.00                I  999.9       0  \n",
       "3                  17.1                  0.41                D  999.9  101000  \n",
       "4               *  32.0               *  0.09                B  999.9       0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import fsspec\n",
    "import pandas as pd\n",
    "\n",
    "# --- Configuration HDFS ---\n",
    "HDFS_BASE_DIR = \"/user/mathis/raw/noaa_gsod\"\n",
    "TARGET_YEAR = 2005\n",
    "\n",
    "# Connexion HDFS\n",
    "fs = fsspec.filesystem(\"hdfs\", host=\"namenode\", port=9000)\n",
    "\n",
    "# 1. Lister les fichiers CSV pour l'ann√©e cible dans HDFS\n",
    "file_pattern = f\"{HDFS_BASE_DIR}/{TARGET_YEAR}/*.csv\"\n",
    "all_files_2005 = fs.glob(file_pattern)\n",
    "\n",
    "if not all_files_2005:\n",
    "    print(f\"‚ùå Aucun fichier CSV trouv√© dans HDFS : {file_pattern}\")\n",
    "\n",
    "else:\n",
    "    # 2. Trier les fichiers par nom (comme en local)\n",
    "    all_files_2005.sort()\n",
    "    first_file_path = all_files_2005[0]\n",
    "    file_name = first_file_path.split(\"/\")[-1]\n",
    "\n",
    "    print(f\"üìÅ Fichier HDFS cibl√© : {first_file_path}\")\n",
    "\n",
    "    # 3. Lecture des 5 premi√®res lignes via pandas\n",
    "    try:\n",
    "        with fs.open(first_file_path, \"rb\") as f:\n",
    "            df_head = pd.read_csv(f, nrows=5)\n",
    "\n",
    "        print(f\"\\n--- HEAD du fichier {file_name} ---\")\n",
    "        display(df_head)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors de la lecture du fichier CSV HDFS : {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bbb709-9985-4754-8f81-3724928ef9cb",
   "metadata": {},
   "source": [
    "Phase 3 : Ingestion Cibl√©e et Persistance (Couche Raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a98c2825-e1c6-450b-a5dd-40d475bcfa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de 552 fichiers existants seront lus par Spark.\n",
      "\n",
      "Sauvegarde de la copie BRUTE filtr√©e (2005-2023) dans : hdfs://namenode:9000/user/mathis/datalake/noaa_gsod_nyc_raw_2005_2023.parquet...\n",
      "‚úÖ Copie brute sauvegard√©e sur HDFS. Le traitement peut se poursuivre.\n"
     ]
    }
   ],
   "source": [
    "import fsspec\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# --- Configuration des chemins ---\n",
    "HDFS_BASE_DIR = \"/user/mathis/raw/noaa_gsod\"\n",
    "RAW_OUTPUT_PATH = \"hdfs://namenode:9000/user/mathis/datalake/noaa_gsod_nyc_raw_2005_2023.parquet\"\n",
    "START_YEAR = 2005\n",
    "END_YEAR = 2023\n",
    "\n",
    "# --- Connexion HDFS ---\n",
    "fs = fsspec.filesystem(\"hdfs\", host=\"namenode\", port=9000)\n",
    "\n",
    "# --- 1. Construction de la liste des fichiers GSOD existants dans HDFS ---\n",
    "gsod_data_paths = []\n",
    "\n",
    "for year in range(START_YEAR, END_YEAR + 1):\n",
    "    for station_id in relevant_station_ids:\n",
    "        hdfs_path = f\"{HDFS_BASE_DIR}/{year}/{station_id}.csv\"\n",
    "        if fs.exists(hdfs_path):\n",
    "            # IMPORTANT : Spark doit recevoir un chemin HDFS complet\n",
    "            gsod_data_paths.append(f\"hdfs://namenode:9000{hdfs_path}\")\n",
    "\n",
    "if not gsod_data_paths:\n",
    "    raise FileNotFoundError(\"‚ùå Aucun fichier GSOD trouv√© dans HDFS.\")\n",
    "\n",
    "print(f\"üìÅ Total de {len(gsod_data_paths)} fichiers GSOD trouv√©s dans HDFS pour lecture Spark.\")\n",
    "\n",
    "# --- 2. Sch√©ma GSOD ---\n",
    "gsod_schema = StructType([\n",
    "    StructField(\"STATION\", StringType(), True),\n",
    "    StructField(\"DATE\", StringType(), True),\n",
    "    StructField(\"LATITUDE\", DoubleType(), True), \n",
    "    StructField(\"LONGITUDE\", DoubleType(), True),\n",
    "    StructField(\"ELEVATION\", DoubleType(), True),\n",
    "    StructField(\"NAME\", StringType(), True),\n",
    "    StructField(\"TEMP\", DoubleType(), True),\n",
    "    StructField(\"TEMP_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"DEWP\", DoubleType(), True),\n",
    "    StructField(\"DEWP_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"SLP\", DoubleType(), True),\n",
    "    StructField(\"SLP_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"STP\", DoubleType(), True),\n",
    "    StructField(\"STP_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"VISIB\", DoubleType(), True),\n",
    "    StructField(\"VISIB_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"WDSP\", DoubleType(), True),\n",
    "    StructField(\"WDSP_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"MXSPD\", DoubleType(), True),\n",
    "    StructField(\"GUST\", DoubleType(), True),\n",
    "    StructField(\"MAX\", DoubleType(), True),\n",
    "    StructField(\"MAX_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"MIN\", DoubleType(), True),\n",
    "    StructField(\"MIN_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"PRCP\", DoubleType(), True),\n",
    "    StructField(\"PRCP_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"SNDP\", DoubleType(), True),\n",
    "    StructField(\"FRSHHT\", StringType(), True),\n",
    "])\n",
    "\n",
    "# --- 3. Lecture Spark directement depuis HDFS ---\n",
    "print(f\"üì• Lecture distribu√©e des fichiers GSOD depuis HDFS...\")\n",
    "\n",
    "all_gsod_data = spark.read.csv(\n",
    "    gsod_data_paths,\n",
    "    header=True,\n",
    "    schema=gsod_schema,\n",
    "    sep=','\n",
    ")\n",
    "\n",
    "nyc_gsod_data = all_gsod_data.withColumnRenamed(\"STATION\", \"ID_STATION\")\n",
    "\n",
    "print(f\"üìä Nombre de lignes charg√©es : {nyc_gsod_data.count()}\")\n",
    "\n",
    "# --- 4. Persistance de la couche brute ---\n",
    "print(f\"\\nüíæ Sauvegarde sur HDFS dans : {RAW_OUTPUT_PATH}\")\n",
    "\n",
    "nyc_gsod_data.write.mode(\"overwrite\").parquet(RAW_OUTPUT_PATH)\n",
    "\n",
    "print(\"‚úÖ Donn√©es GSOD NYC brutes sauvegard√©es dans HDFS.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370c4c89-1bbb-4ded-b71d-cdd10ad0b18c",
   "metadata": {},
   "source": [
    "Phase 4 : Nettoyage et Jointure des Coordonn√©es (ETL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fbff00b-d1ef-4cd7-8f69-e8b51dea4693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Donn√©es NOAA nettoy√©es et enrichies des coordonn√©es des stations (Couche ETL Compl√®te).\n",
      "root\n",
      " |-- ID_STATION: string (nullable = true)\n",
      " |-- DATE_OBSERVATION: string (nullable = true)\n",
      " |-- ELEVATION: double (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- TEMP_MOYENNE_F: double (nullable = true)\n",
      " |-- TEMP_ATTRIBUTES: string (nullable = true)\n",
      " |-- POINT_ROSEE_F: double (nullable = true)\n",
      " |-- DEWP_ATTRIBUTES: string (nullable = true)\n",
      " |-- PRESSION_ATM_MER: double (nullable = true)\n",
      " |-- SLP_ATTRIBUTES: string (nullable = true)\n",
      " |-- STP: double (nullable = true)\n",
      " |-- STP_ATTRIBUTES: string (nullable = true)\n",
      " |-- VISIBILITE_MILLES: double (nullable = true)\n",
      " |-- VISIB_ATTRIBUTES: string (nullable = true)\n",
      " |-- VITESSE_VENT_NOEUDS: double (nullable = true)\n",
      " |-- WDSP_ATTRIBUTES: string (nullable = true)\n",
      " |-- MXSPD: double (nullable = true)\n",
      " |-- GUST: double (nullable = true)\n",
      " |-- TEMP_MAX_F: double (nullable = true)\n",
      " |-- MAX_ATTRIBUTES: string (nullable = true)\n",
      " |-- TEMP_MIN_F: double (nullable = true)\n",
      " |-- MIN_ATTRIBUTES: string (nullable = true)\n",
      " |-- PRECIPITATION_POUCES: double (nullable = true)\n",
      " |-- PRCP_ATTRIBUTES: string (nullable = true)\n",
      " |-- SNDP: double (nullable = true)\n",
      " |-- PHENOMENES: string (nullable = true)\n",
      " |-- LATITUDE_STATION: double (nullable = true)\n",
      " |-- LONGITUDE_STATION: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# --- Chemins de configuration ---\n",
    "RAW_OUTPUT_PATH = \"hdfs://namenode:9000/user/mathis/datalake/noaa_gsod_nyc_raw_2005_2023.parquet\"\n",
    "# nyc_stations_spark doit √™tre le DataFrame des stations filtr√©es de la Phase 2\n",
    "\n",
    "# Lecture du fichier Parquet depuis HDFS pour la couche de Traitement/Nettoyage (ETL)\n",
    "data_for_cleaning = spark.read.parquet(RAW_OUTPUT_PATH)\n",
    "\n",
    "# --- 1. Nettoyage et Renommage des Colonnes ---\n",
    "renaming_map = {\n",
    "    \"DATE\": \"DATE_OBSERVATION\",\n",
    "    \"TEMP\": \"TEMP_MOYENNE_F\",\n",
    "    \"DEWP\": \"POINT_ROSEE_F\",\n",
    "    \"PRCP\": \"PRECIPITATION_POUCES\",\n",
    "    \"MIN\": \"TEMP_MIN_F\",\n",
    "    \"MAX\": \"TEMP_MAX_F\",\n",
    "    \"WDSP\": \"VITESSE_VENT_NOEUDS\",\n",
    "    \"VISIB\": \"VISIBILITE_MILLES\",\n",
    "    \"SLP\": \"PRESSION_ATM_MER\",\n",
    "    \"FRSHHT\": \"PHENOMENES\"\n",
    "}\n",
    "\n",
    "gsod_renamed_df = data_for_cleaning\n",
    "for old_name, new_name in renaming_map.items():\n",
    "    if old_name in gsod_renamed_df.columns:\n",
    "        gsod_renamed_df = gsod_renamed_df.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "\n",
    "# --- 2. Jointure des Coordonn√©es des Stations ---\n",
    "# Ajoute la LATITUDE_STATION et LONGITUDE_STATION pour le calcul de distance\n",
    "clean_final_gsod_df = gsod_renamed_df.join(\n",
    "    nyc_stations_spark.select(\n",
    "        'STN_ID', \n",
    "        F.col('LATITUDE').alias('LATITUDE_STATION'), \n",
    "        F.col('LONGITUDE').alias('LONGITUDE_STATION')\n",
    "    ), \n",
    "    on=[F.col(\"ID_STATION\") == F.col(\"STN_ID\")],\n",
    "    how='left'\n",
    ").drop(\"STN_ID\").drop(\"LATITUDE\").drop(\"LONGITUDE\") # Supprime les colonnes brutes des coordonn√©es\n",
    "\n",
    "\n",
    "print(\"\\nDonn√©es NOAA nettoy√©es et enrichies des coordonn√©es des stations (Couche ETL Compl√®te).\")\n",
    "clean_final_gsod_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca3cea98-3dc1-4c4f-918a-a0a1149988d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema des donn√©es brutes (193170 enregistrements) :\n",
      "root\n",
      " |-- ID_STATION: string (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      " |-- ELEVATION: double (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- TEMP: double (nullable = true)\n",
      " |-- TEMP_ATTRIBUTES: string (nullable = true)\n",
      " |-- DEWP: double (nullable = true)\n",
      " |-- DEWP_ATTRIBUTES: string (nullable = true)\n",
      " |-- SLP: double (nullable = true)\n",
      " |-- SLP_ATTRIBUTES: string (nullable = true)\n",
      " |-- STP: double (nullable = true)\n",
      " |-- STP_ATTRIBUTES: string (nullable = true)\n",
      " |-- VISIB: double (nullable = true)\n",
      " |-- VISIB_ATTRIBUTES: string (nullable = true)\n",
      " |-- WDSP: double (nullable = true)\n",
      " |-- WDSP_ATTRIBUTES: string (nullable = true)\n",
      " |-- MXSPD: double (nullable = true)\n",
      " |-- GUST: double (nullable = true)\n",
      " |-- MAX: double (nullable = true)\n",
      " |-- MAX_ATTRIBUTES: string (nullable = true)\n",
      " |-- MIN: double (nullable = true)\n",
      " |-- MIN_ATTRIBUTES: string (nullable = true)\n",
      " |-- PRCP: double (nullable = true)\n",
      " |-- PRCP_ATTRIBUTES: string (nullable = true)\n",
      " |-- SNDP: double (nullable = true)\n",
      " |-- FRSHHT: string (nullable = true)\n",
      "\n",
      "\n",
      "--- HEAD des Donn√©es Brutes Filtr√©es (NOAA 2005-2023) ---\n",
      "+-----------+----------+--------------------+----+-----+----+------+------+\n",
      "|ID_STATION |DATE      |NAME                |TEMP|PRCP |DEWP|SLP   |FRSHHT|\n",
      "+-----------+----------+--------------------+----+-----+----+------+------+\n",
      "|72503814714|2008-01-01|STEWART FIELD, NY US|31.6|99.99|28.6|9999.9|101000|\n",
      "|72503814714|2008-01-02|STEWART FIELD, NY US|28.4|0.0  |18.0|9999.9|000000|\n",
      "|72503814714|2008-01-03|STEWART FIELD, NY US|12.0|0.0  |-1.0|9999.9|000000|\n",
      "|72503814714|2008-01-04|STEWART FIELD, NY US|20.1|0.0  |9.1 |9999.9|000000|\n",
      "|72503814714|2008-01-05|STEWART FIELD, NY US|32.2|0.0  |16.5|9999.9|000000|\n",
      "+-----------+----------+--------------------+----+-----+----+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# --- Chemin de la Couche Raw sur HDFS ---\n",
    "RAW_OUTPUT_PATH = \"hdfs://namenode:9000/user/mathis/datalake/noaa_gsod_nyc_raw_2005_2023.parquet\"\n",
    "\n",
    "# Recharger la session Spark si elle a √©t√© arr√™t√©e\n",
    "spark = SparkSession.builder.appName(\"Check_Raw_Data\").getOrCreate()\n",
    "\n",
    "try:\n",
    "    # 1. Lecture du DataFrame Brute depuis HDFS\n",
    "    df_raw_check = spark.read.parquet(RAW_OUTPUT_PATH)\n",
    "    \n",
    "    # Renommage de la colonne STATION pour la lisibilit√©\n",
    "    df_raw_check = df_raw_check.withColumnRenamed(\"STATION\", \"ID_STATION\")\n",
    "    \n",
    "    print(f\"Schema des donn√©es brutes ({df_raw_check.count()} enregistrements) :\")\n",
    "    df_raw_check.printSchema()\n",
    "    \n",
    "    # 2. Affichage des 5 premi√®res lignes\n",
    "    # On affiche les colonnes cl√©s (ID, Date, Temp√©rature, Pr√©cipitation)\n",
    "    print(\"\\n--- HEAD des Donn√©es Brutes Filtr√©es (NOAA 2005-2023) ---\")\n",
    "    df_raw_check.select(\n",
    "        \"ID_STATION\",\n",
    "        \"DATE\",\n",
    "        \"NAME\",\n",
    "        \"TEMP\",\n",
    "        \"PRCP\",\n",
    "        \"DEWP\",\n",
    "        \"SLP\",\n",
    "        \"FRSHHT\"\n",
    "    ).show(5, truncate=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERREUR lors de la lecture du fichier HDFS : {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e4e0e7-f52e-47a2-886d-e7e2df0e82c7",
   "metadata": {},
   "source": [
    "Phase 5 : Pr√©paration des Coordonn√©es des Zones de Qualit√© de l'Air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57332368-11f2-4340-880e-e64438374d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "43 zones de qualit√© de l'air (GeoJoin ID) pr√©par√©es avec leurs coordonn√©es.\n",
      "\n",
      "Premi√®res lignes du DataFrame des coordonn√©es g√©ographiques :\n",
      "+----------+-------------+--------------+\n",
      "|GEOJOIN_ID|LATITUDE_ZONE|LONGITUDE_ZONE|\n",
      "+----------+-------------+--------------+\n",
      "|       101|    40.905562|    -73.877928|\n",
      "|       102|    40.889002|    -73.815044|\n",
      "|       103|     40.88315|    -73.856266|\n",
      "|       104|    40.821533|    -73.883659|\n",
      "|       105|    40.859781|    -73.914444|\n",
      "+----------+-------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 1. T√©l√©chargement et Extraction du GeoJSON ---\n",
    "geojson_url = \"https://raw.githubusercontent.com/nycehs/NYC_geography/master/UHF42.geo.json\"\n",
    "\n",
    "response = requests.get(geojson_url)\n",
    "geo_data_raw = response.json()\n",
    "\n",
    "geo_records = []\n",
    "for feature in geo_data_raw['features']:\n",
    "    properties = feature['properties']\n",
    "    geometry = feature['geometry']\n",
    "    coords = geometry['coordinates']\n",
    "    \n",
    "    try:\n",
    "        # Approximation du centro√Øde pour la distance\n",
    "        if geometry['type'] == 'Polygon':\n",
    "            lon = coords[0][0][0]\n",
    "            lat = coords[0][0][1]\n",
    "        elif geometry['type'] == 'MultiPolygon':\n",
    "            lon = coords[0][0][0][0]\n",
    "            lat = coords[0][0][0][1]\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        geo_records.append({\n",
    "            \"GEOJOIN_ID\": properties['GEOCODE'], \n",
    "            \"LATITUDE_ZONE\": lat,\n",
    "            \"LONGITUDE_ZONE\": lon\n",
    "        })\n",
    "    except (IndexError, TypeError):\n",
    "        continue\n",
    "\n",
    "# --- 2. Cr√©ation du DataFrame Spark des Coordonn√©es des Zones ---\n",
    "geo_schema = StructType([\n",
    "    StructField(\"GEOJOIN_ID\", StringType(), False),\n",
    "    StructField(\"LATITUDE_ZONE\", DoubleType(), True),\n",
    "    StructField(\"LONGITUDE_ZONE\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "geo_df = spark.createDataFrame(geo_records, schema=geo_schema)\n",
    "\n",
    "print(f\"\\n{geo_df.count()} zones de qualit√© de l'air (GeoJoin ID) pr√©par√©es avec leurs coordonn√©es.\")\n",
    "\n",
    "# --- Affichage des premi√®res lignes du DataFrame ---\n",
    "print(\"\\nPremi√®res lignes du DataFrame des coordonn√©es g√©ographiques :\")\n",
    "geo_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8569e1-fa8b-4b64-8861-9643bf5c66b1",
   "metadata": {},
   "source": [
    "Phase 6 : Calcul de la Distance (Haversine) et Voisin le Plus Proche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2aa5b600-9b82-46af-954f-f6a5f2377bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Table de correspondance (Voisin le plus proche) cr√©√©e :\n",
      "+----------+-----------+------------------+\n",
      "|GEOJOIN_ID|ID_STATION |DISTANCE_KM       |\n",
      "+----------+-----------+------------------+\n",
      "|0         |74486094789|5.760247595051896 |\n",
      "|101       |72503014732|14.074131616303024|\n",
      "|102       |99728099999|9.640847117034312 |\n",
      "|103       |99728099999|11.103908014210129|\n",
      "|104       |72503014732|4.739471712784995 |\n",
      "+----------+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Constante du rayon de la Terre en kilom√®tres (km)\n",
    "R = 6371.0\n",
    "\n",
    "# Formule de la Haversine (UDF)\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    # Conversion de degr√©s en radians\n",
    "    lon1_rad, lat1_rad, lon2_rad, lat2_rad = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1_rad) * cos(lat2_rad) * sin(dlon / 2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    \n",
    "    return R * c\n",
    "\n",
    "# Enregistrement de la fonction comme une UDF (User Defined Function) pour Spark\n",
    "haversine_udf = F.udf(haversine, DoubleType())\n",
    "\n",
    "# --- 1. Pr√©paration des Coordonn√©es des Stations Uniques ---\n",
    "stations_coords_df = clean_final_gsod_df.select(\n",
    "    \"ID_STATION\", \"LATITUDE_STATION\", \"LONGITUDE_STATION\"\n",
    ").distinct()\n",
    "\n",
    "# --- 2. Jointure Cart√©sienne (Toutes les zones vs. Toutes les stations) ---\n",
    "cross_joined_df = geo_df.crossJoin(stations_coords_df)\n",
    "\n",
    "\n",
    "# --- 3. Calcul de la Distance pour chaque paire ---\n",
    "distance_df = cross_joined_df.withColumn(\n",
    "    \"DISTANCE_KM\",\n",
    "    haversine_udf(\n",
    "        F.col(\"LONGITUDE_ZONE\"), \n",
    "        F.col(\"LATITUDE_ZONE\"), \n",
    "        F.col(\"LONGITUDE_STATION\"), \n",
    "        F.col(\"LATITUDE_STATION\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- 4. Identification du Voisin le Plus Proche ---\n",
    "# Trouve la ligne (station) avec la distance minimale pour chaque GEOJOIN_ID\n",
    "window_spec = Window.partitionBy(\"GEOJOIN_ID\").orderBy(F.col(\"DISTANCE_KM\"))\n",
    "\n",
    "nearest_station_df = distance_df.withColumn(\n",
    "    \"rank\", \n",
    "    F.rank().over(window_spec)\n",
    ").filter(F.col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "\n",
    "print(\"\\n‚úÖ Table de correspondance (Voisin le plus proche) cr√©√©e :\")\n",
    "nearest_station_df.select(\n",
    "    \"GEOJOIN_ID\", \n",
    "    \"ID_STATION\", \n",
    "    \"DISTANCE_KM\"\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eb9645-f14c-4e47-a2b1-6fa6b78e1d55",
   "metadata": {},
   "source": [
    "Phase 7 : Jointure Finale et Persistance (Couche Insight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f91da9d-f8dc-452d-a43c-7efd1e98a01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Structure du DataFrame Final (M√©t√©o attribu√©e √† chaque zone de qualit√© de l'air) :\n",
      "root\n",
      " |-- GEOJOIN_ID: string (nullable = false)\n",
      " |-- DATE_OBSERVATION: string (nullable = true)\n",
      " |-- TEMP_MOYENNE_F: double (nullable = true)\n",
      " |-- PRECIPITATION_POUCES: double (nullable = true)\n",
      " |-- VITESSE_VENT_NOEUDS: double (nullable = true)\n",
      " |-- PHENOMENES: string (nullable = true)\n",
      " |-- ID_STATION: string (nullable = true)\n",
      " |-- DISTANCE_KM: double (nullable = true)\n",
      " |-- LATITUDE_ZONE: double (nullable = true)\n",
      " |-- LONGITUDE_ZONE: double (nullable = true)\n",
      "\n",
      "+----------+----------------+--------------+--------------------+-------------------+----------+-----------+------------------+-------------+--------------+\n",
      "|GEOJOIN_ID|DATE_OBSERVATION|TEMP_MOYENNE_F|PRECIPITATION_POUCES|VITESSE_VENT_NOEUDS|PHENOMENES| ID_STATION|       DISTANCE_KM|LATITUDE_ZONE|LONGITUDE_ZONE|\n",
      "+----------+----------------+--------------+--------------------+-------------------+----------+-----------+------------------+-------------+--------------+\n",
      "|       405|      2008-01-01|          41.9|                0.15|                9.9|    010000|72503014732| 8.400119154879857|    40.715308|    -73.826379|\n",
      "|       402|      2008-01-01|          41.9|                0.15|                9.9|    010000|72503014732| 3.757173548253043|    40.765055|     -73.83936|\n",
      "|       401|      2008-01-01|          41.9|                0.15|                9.9|    010000|72503014732|1.8621544795208718|    40.780124|    -73.902066|\n",
      "|       107|      2008-01-01|          41.9|                0.15|                9.9|    010000|72503014732| 6.161877294189115|    40.833646|    -73.892155|\n",
      "|       106|      2008-01-01|          41.9|                0.15|                9.9|    010000|72503014732| 8.039508612144148|    40.846455|    -73.914385|\n",
      "+----------+----------------+--------------+--------------------+-------------------+----------+-----------+------------------+-------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Sauvegarde du DataFrame final Nettoy√©/Enrichi (Couche Insight) dans : hdfs://namenode:9000/user/mathis/datalake/noaa_gsod_nyc_enriched_clean.parquet...\n",
      "‚úÖ Jointure finale et persistance termin√©es. Le jeu de donn√©es m√©t√©o est pr√™t pour l'analyse.\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# --- Configuration HDFS ---\n",
    "# Chemin o√π sera stock√© le jeu de donn√©es final, pr√™t pour l'analyse\n",
    "INSIGHT_OUTPUT_PATH = \"hdfs://namenode:9000/user/mathis/datalake/noaa_gsod_nyc_enriched_clean.parquet\"\n",
    "\n",
    "# --- 1. Jointure d'Enrichissement ---\n",
    "# On joint toutes les observations m√©t√©o (clean_final_gsod_df) aux informations de GeoJoin ID (nearest_station_df)\n",
    "# La jointure s'effectue sur l'ID de la station m√©t√©o.\n",
    "\n",
    "# NOTE: Ce bloc n√©cessite d'avoir ex√©cut√© la Phase 6 juste avant pour que 'nearest_station_df'\n",
    "# et 'clean_final_gsod_df' soient disponibles en m√©moire.\n",
    "\n",
    "final_insights_df = clean_final_gsod_df.join(\n",
    "    nearest_station_df.select(\"GEOJOIN_ID\", \n",
    "                              F.col(\"ID_STATION\").alias(\"NEAREST_STATION_ID\"), \n",
    "                              \"DISTANCE_KM\",\n",
    "                              \"LATITUDE_ZONE\", \n",
    "                              \"LONGITUDE_ZONE\"),\n",
    "    # La condition est que l'ID de la station m√©t√©o DOIT correspondre √† l'ID du voisin le plus proche.\n",
    "    F.col(\"ID_STATION\") == F.col(\"NEAREST_STATION_ID\"),\n",
    "    \"inner\" \n",
    ").drop(\"NEAREST_STATION_ID\") # On retire cette colonne apr√®s la jointure\n",
    "\n",
    "\n",
    "# --- 2. S√©lection et Ordre Final ---\n",
    "# Cr√©e le sch√©ma final pour l'analyse, en pla√ßant les cl√©s d'analyse en t√™te\n",
    "final_insights_df = final_insights_df.select(\n",
    "    \"GEOJOIN_ID\", \n",
    "    \"DATE_OBSERVATION\", \n",
    "    # Mesures M√©t√©o\n",
    "    \"TEMP_MOYENNE_F\", \n",
    "    \"PRECIPITATION_POUCES\", \n",
    "    \"VITESSE_VENT_NOEUDS\", \n",
    "    \"PHENOMENES\",\n",
    "    # M√©ta-donn√©es de jointure\n",
    "    \"ID_STATION\", \n",
    "    \"DISTANCE_KM\",\n",
    "    \"LATITUDE_ZONE\", \n",
    "    \"LONGITUDE_ZONE\" # Coordonn√©es de la zone (utile pour la cartographie)\n",
    ")\n",
    "\n",
    "print(\"\\nStructure du DataFrame Final (M√©t√©o attribu√©e √† chaque zone de qualit√© de l'air) :\")\n",
    "final_insights_df.printSchema()\n",
    "final_insights_df.show(5)\n",
    "\n",
    "# --- 3. Persistance de la Couche Insight sur HDFS ---\n",
    "print(f\"\\nSauvegarde du DataFrame final Nettoy√©/Enrichi (Couche Insight) dans : {INSIGHT_OUTPUT_PATH}...\")\n",
    "# Sauvegarde au format Parquet\n",
    "final_insights_df.write.mode(\"overwrite\").parquet(INSIGHT_OUTPUT_PATH)\n",
    "print(\"‚úÖ Jointure finale et persistance termin√©es. Le jeu de donn√©es m√©t√©o est pr√™t pour l'analyse.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42c280-cadc-4428-89f5-4888d8c3f821",
   "metadata": {},
   "source": [
    "Phase 8 (A) : T√©l√©chargement et Nettoyage du JSON Socrata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "958651e3-8c24-4c8d-9bd4-53aadc061f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚¨áÔ∏è T√©l√©chargement du JSON Socrata depuis l'API de NYC...\n",
      "‚úÖ Fichier JSON brut sauvegard√© et nettoy√© structurellement √† : /home/jovyan/work/data/air_quality/nyc_air_quality_raw.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "# --- Configuration des Chemins ---\n",
    "LOCAL_BASE_DIR = \"/home/jovyan/work/data/air_quality\"\n",
    "LOCAL_JSON_PATH = os.path.join(LOCAL_BASE_DIR, \"nyc_air_quality_raw.json\")\n",
    "AIR_QUALITY_URL = \"https://data.cityofnewyork.us/api/views/c3uy-2p5r/rows.json?accessType=DOWNLOAD\"\n",
    "\n",
    "# Cr√©e le r√©pertoire local si n√©cessaire\n",
    "os.makedirs(LOCAL_BASE_DIR, exist_ok=True)\n",
    "\n",
    "# --- 1. T√©l√©chargement et Nettoyage de la structure JSON Socrata ---\n",
    "print(f\"‚¨áÔ∏è T√©l√©chargement du JSON Socrata depuis l'API de NYC...\")\n",
    "try:\n",
    "    response = requests.get(AIR_QUALITY_URL, timeout=300) # Timeout de 5 minutes\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    \n",
    "    # La cl√© 'data' contient le tableau des enregistrements bruts que Spark doit lire.\n",
    "    raw_records = data.get('data', [])\n",
    "\n",
    "    if not raw_records:\n",
    "        print(\"‚ùå Erreur : La cl√© 'data' est vide dans le JSON t√©l√©charg√©. Arr√™t du processus.\")\n",
    "        exit()\n",
    "    \n",
    "    # √âcriture du tableau de donn√©es brutes SEULEMENT dans le nouveau fichier JSON.\n",
    "    # Ceci est essentiel pour que le RDD/toDF fonctionne correctement.\n",
    "    with open(LOCAL_JSON_PATH, 'w') as f:\n",
    "        json.dump(raw_records, f)\n",
    "\n",
    "    print(f\"‚úÖ Fichier JSON brut sauvegard√© et nettoy√© structurellement √† : {LOCAL_JSON_PATH}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors du t√©l√©chargement/nettoyage : {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1388c06-a297-40ce-9bc2-712369dbc6bf",
   "metadata": {},
   "source": [
    "Phase 8 (B) : Ingestion, Normalisation et Pivotage de la Qualit√© de l'Air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebae8573-6836-4380-bcb9-efe730824991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qualit√© de l'air : 18862 enregistrements bruts lus.\n",
      "root\n",
      " |-- col_0: string (nullable = true)\n",
      " |-- col_1: string (nullable = true)\n",
      " |-- col_2: string (nullable = true)\n",
      " |-- col_3: string (nullable = true)\n",
      " |-- col_4: string (nullable = true)\n",
      " |-- col_5: string (nullable = true)\n",
      " |-- col_6: string (nullable = true)\n",
      " |-- col_7: string (nullable = true)\n",
      " |-- col_8: string (nullable = true)\n",
      " |-- col_9: string (nullable = true)\n",
      " |-- NOM_POLLUANT: string (nullable = true)\n",
      " |-- col_11: string (nullable = true)\n",
      " |-- col_12: string (nullable = true)\n",
      " |-- col_13: string (nullable = true)\n",
      " |-- GEOJOIN_ID_BRUT: string (nullable = true)\n",
      " |-- col_15: string (nullable = true)\n",
      " |-- col_16: string (nullable = true)\n",
      " |-- DATE_MESURE_BRUTE: string (nullable = true)\n",
      " |-- VALEUR_MESURE_BRUTE: string (nullable = true)\n",
      " |-- col_19: string (nullable = true)\n",
      "\n",
      "\n",
      "Structure pivot√©e des donn√©es de qualit√© de l'air (pr√™te pour la jointure finale) :\n",
      "root\n",
      " |-- GEOJOIN_ID: string (nullable = true)\n",
      " |-- DATE_OBSERVATION: integer (nullable = true)\n",
      " |-- QA_Annual_vehicle_miles_traveled_MOYENNE: double (nullable = true)\n",
      " |-- QA_Annual_vehicle_miles_traveled_cars_MOYENNE: double (nullable = true)\n",
      " |-- QA_Annual_vehicle_miles_traveled_trucks_MOYENNE: double (nullable = true)\n",
      " |-- QA_Asthma_emergency_department_visits_due_to_PM2_5_MOYENNE: double (nullable = true)\n",
      " |-- QA_Asthma_emergency_departments_visits_due_to_Ozone_MOYENNE: double (nullable = true)\n",
      " |-- QA_Asthma_hospitalizations_due_to_Ozone_MOYENNE: double (nullable = true)\n",
      " |-- QA_Boiler_Emissions-_Total_NOx_Emissions_MOYENNE: double (nullable = true)\n",
      " |-- QA_Boiler_Emissions-_Total_PM2_5_Emissions_MOYENNE: double (nullable = true)\n",
      " |-- QA_Boiler_Emissions-_Total_SO2_Emissions_MOYENNE: double (nullable = true)\n",
      " |-- QA_Cardiac_and_respiratory_deaths_due_to_Ozone_MOYENNE: double (nullable = true)\n",
      " |-- QA_Cardiovascular_hospitalizations_due_to_PM2_5_age_40+_MOYENNE: double (nullable = true)\n",
      " |-- QA_Deaths_due_to_PM2_5_MOYENNE: double (nullable = true)\n",
      " |-- QA_Fine_particles_PM_2_5_MOYENNE: double (nullable = true)\n",
      " |-- QA_Nitrogen_dioxide_NO2_MOYENNE: double (nullable = true)\n",
      " |-- QA_Outdoor_Air_Toxics_-_Benzene_MOYENNE: double (nullable = true)\n",
      " |-- QA_Outdoor_Air_Toxics_-_Formaldehyde_MOYENNE: double (nullable = true)\n",
      " |-- QA_Ozone_O3_MOYENNE: double (nullable = true)\n",
      " |-- QA_Respiratory_hospitalizations_due_to_PM2_5_age_20+_MOYENNE: double (nullable = true)\n",
      "\n",
      "+----------+----------------+----------------------------------------+---------------------------------------------+-----------------------------------------------+----------------------------------------------------------+-----------------------------------------------------------+-----------------------------------------------+------------------------------------------------+--------------------------------------------------+------------------------------------------------+------------------------------------------------------+---------------------------------------------------------------+------------------------------+--------------------------------+-------------------------------+---------------------------------------+--------------------------------------------+-------------------+------------------------------------------------------------+\n",
      "|GEOJOIN_ID|DATE_OBSERVATION|QA_Annual_vehicle_miles_traveled_MOYENNE|QA_Annual_vehicle_miles_traveled_cars_MOYENNE|QA_Annual_vehicle_miles_traveled_trucks_MOYENNE|QA_Asthma_emergency_department_visits_due_to_PM2_5_MOYENNE|QA_Asthma_emergency_departments_visits_due_to_Ozone_MOYENNE|QA_Asthma_hospitalizations_due_to_Ozone_MOYENNE|QA_Boiler_Emissions-_Total_NOx_Emissions_MOYENNE|QA_Boiler_Emissions-_Total_PM2_5_Emissions_MOYENNE|QA_Boiler_Emissions-_Total_SO2_Emissions_MOYENNE|QA_Cardiac_and_respiratory_deaths_due_to_Ozone_MOYENNE|QA_Cardiovascular_hospitalizations_due_to_PM2_5_age_40+_MOYENNE|QA_Deaths_due_to_PM2_5_MOYENNE|QA_Fine_particles_PM_2_5_MOYENNE|QA_Nitrogen_dioxide_NO2_MOYENNE|QA_Outdoor_Air_Toxics_-_Benzene_MOYENNE|QA_Outdoor_Air_Toxics_-_Formaldehyde_MOYENNE|QA_Ozone_O3_MOYENNE|QA_Respiratory_hospitalizations_due_to_PM2_5_age_20+_MOYENNE|\n",
      "+----------+----------------+----------------------------------------+---------------------------------------------+-----------------------------------------------+----------------------------------------------------------+-----------------------------------------------------------+-----------------------------------------------+------------------------------------------------+--------------------------------------------------+------------------------------------------------+------------------------------------------------------+---------------------------------------------------------------+------------------------------+--------------------------------+-------------------------------+---------------------------------------+--------------------------------------------+-------------------+------------------------------------------------------------+\n",
      "|       503|        20210101|                                    NULL|                                         NULL|                                           NULL|                                                      NULL|                                                       NULL|                                           NULL|                                            NULL|                                              NULL|                                            NULL|                                                  NULL|                                                           NULL|                          NULL|                    6.1042113675|                    12.18993265|                                   NULL|                                        NULL|               NULL|                                                        NULL|\n",
      "| 105106107|        20210101|                                    NULL|                                         NULL|                                           NULL|                                                      NULL|                                                       NULL|                                           NULL|                                            NULL|                                              NULL|                                            NULL|                                                  NULL|                                                           NULL|                          NULL|                     6.826255998|                    18.11753712|                                   NULL|                                        NULL|               NULL|                                                        NULL|\n",
      "|       503|        20120601|                                    NULL|                                         NULL|                                           NULL|                                                      NULL|                                                       NULL|                                           NULL|                                            NULL|                                              NULL|                                            NULL|                                                  NULL|                                                           NULL|                          NULL|                            9.96|                          9.905|                                   NULL|                                        NULL|              34.01|                                                        NULL|\n",
      "|       403|        20221201|                                    NULL|                                         NULL|                                           NULL|                                                      NULL|                                                       NULL|                                           NULL|                                            NULL|                                              NULL|                                            NULL|                                                  NULL|                                                           NULL|                          NULL|               6.305303207000001|             19.574104493333333|                                   NULL|                                        NULL|               NULL|                                                        NULL|\n",
      "|       501|        20090601|                                    NULL|                                         NULL|                                           NULL|                                                      NULL|                                                       NULL|                                           NULL|                                            NULL|                                              NULL|                                            NULL|                                                  NULL|                                                           NULL|                          NULL|                          10.665|                          17.82|                                   NULL|                                        NULL| 24.645000000000003|                                                        NULL|\n",
      "+----------+----------------+----------------------------------------+---------------------------------------------+-----------------------------------------------+----------------------------------------------------------+-----------------------------------------------------------+-----------------------------------------------+------------------------------------------------+--------------------------------------------------+------------------------------------------------+------------------------------------------------------+---------------------------------------------------------------+------------------------------+--------------------------------+-------------------------------+---------------------------------------+--------------------------------------------+-------------------+------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "‚úÖ Qualit√© de l'air pivot√©e et persist√©e sur HDFS √† : hdfs://namenode:9000/user/mathis/datalake/nyc_air_quality_pivot.parquet\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "# --- Configurations des Chemins ---\n",
    "LOCAL_BASE_DIR = \"/home/jovyan/work/data/air_quality\"\n",
    "LOCAL_JSON_PATH = os.path.join(LOCAL_BASE_DIR, \"nyc_air_quality_raw.json\")\n",
    "AIR_QUALITY_URL = \"https://data.cityofnewyork.us/api/views/c3uy-2p5r/rows.json?accessType=DOWNLOAD\"\n",
    "AIR_QUALITY_OUTPUT_PATH = \"hdfs://namenode:9000/user/mathis/datalake/nyc_air_quality_pivot.parquet\"\n",
    "\n",
    "# Cr√©e le r√©pertoire local si n√©cessaire\n",
    "os.makedirs(LOCAL_BASE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- 1. Lecture du tableau JSON en m√©moire Python (Assum√©e r√©ussie) ---\n",
    "# Ce bloc d√©pend du succ√®s de l'√©tape de t√©l√©chargement et de nettoyage du JSON brut.\n",
    "with open(LOCAL_JSON_PATH, 'r') as f:\n",
    "    raw_socrata_array = json.load(f)\n",
    "\n",
    "# --- 2. D√©finition du Sch√©ma RDD/toDF : Correction du format Socrata ---\n",
    "\n",
    "# Mappage des index Socrata confirm√©s (Index dans le tableau data)\n",
    "column_names = {\n",
    "    17: \"DATE_MESURE_BRUTE\",         # Index 17 -> Start_Date\n",
    "    14: \"GEOJOIN_ID_BRUT\",           # Index 14 -> Geo Join ID\n",
    "    10: \"NOM_POLLUANT\",              # Index 10 -> Name (Polluant)\n",
    "    18: \"VALEUR_MESURE_BRUTE\"        # Index 18 -> Data Value\n",
    "}\n",
    "\n",
    "# D√©finir tous les noms de colonnes jusqu'√† l'indice max (19)\n",
    "# La correction est ici : range(20) pour obtenir 20 colonnes (0 √† 19)\n",
    "col_labels = [f\"col_{i}\" for i in range(20)] # ANCIEN: range(19)\n",
    "\n",
    "# Mappage des indices Socrata confirm√©s (inchang√©, ils sont tous < 20)\n",
    "column_names = {\n",
    "    17: \"DATE_MESURE_BRUTE\",\n",
    "    14: \"GEOJOIN_ID_BRUT\",\n",
    "    10: \"NOM_POLLUANT\",\n",
    "    18: \"VALEUR_MESURE_BRUTE\"\n",
    "}\n",
    "\n",
    "# (Le reste du mappage est inchang√©)\n",
    "for index, name in column_names.items():\n",
    "    col_labels[index] = name\n",
    "\n",
    "# CR√âATION DU SCH√âMA STRUCTUR√â (Tous en StringType)\n",
    "target_schema = StructType([\n",
    "    StructField(name, StringType(), True) for name in col_labels\n",
    "])\n",
    "\n",
    "# Cr√©ation du DataFrame Spark √† partir du RDD avec le sch√©ma explicite\n",
    "air_quality_data_raw = spark.sparkContext.parallelize(raw_socrata_array).toDF(target_schema)\n",
    "\n",
    "print(f\"Qualit√© de l'air : {air_quality_data_raw.count()} enregistrements bruts lus.\")\n",
    "air_quality_data_raw.printSchema()\n",
    "\n",
    "\n",
    "# --- 3. S√©lection et Nettoyage des Colonnes (ETL) ---\n",
    "\n",
    "air_quality_df = air_quality_data_raw.select(\n",
    "    \"DATE_MESURE_BRUTE\",\n",
    "    \"GEOJOIN_ID_BRUT\",\n",
    "    \"NOM_POLLUANT\",\n",
    "    \"VALEUR_MESURE_BRUTE\"\n",
    ")\n",
    "\n",
    "air_quality_clean = air_quality_df.filter(\n",
    "    F.col(\"VALEUR_MESURE_BRUTE\").isNotNull()\n",
    ").withColumn(\n",
    "    # Conversion de la valeur de mesure en Double\n",
    "    \"VALEUR_MESURE\", F.col(\"VALEUR_MESURE_BRUTE\").cast(DoubleType())\n",
    ").withColumn(\n",
    "    # GeoJoin ID en String pour la jointure\n",
    "    \"GEOJOIN_ID\", F.col(\"GEOJOIN_ID_BRUT\").cast(StringType())\n",
    ").withColumn(\n",
    "    # Conversion de la date (YYYY-MM-DD...) en format YYYYMMDD entier pour la jointure avec NOAA.\n",
    "    \"DATE_OBSERVATION\", \n",
    "    F.regexp_replace(F.substring(F.col(\"DATE_MESURE_BRUTE\"), 1, 10), \"-\", \"\").cast(IntegerType())\n",
    ").drop(\"DATE_MESURE_BRUTE\", \"VALEUR_MESURE_BRUTE\", \"GEOJOIN_ID_BRUT\")\n",
    "\n",
    "# --- 4. Croisement (Pivot) des Donn√©es de Qualit√© de l'Air ---\n",
    "# Transforme les lignes de polluants en colonnes\n",
    "air_quality_pivot = air_quality_clean.groupBy(\"GEOJOIN_ID\", \"DATE_OBSERVATION\").pivot(\"NOM_POLLUANT\").agg(\n",
    "    F.mean(\"VALEUR_MESURE\")\n",
    ")\n",
    "\n",
    "# Renommage des colonnes pivot√©es pour la clart√©\n",
    "for col_name in air_quality_pivot.columns:\n",
    "    if col_name not in [\"GEOJOIN_ID\", \"DATE_OBSERVATION\"]:\n",
    "        # Nettoyage du nom du polluant\n",
    "        new_col_name = f\"QA_{col_name.replace(' ', '_').replace('.', '_').replace('(', '').replace(')', '').replace('/', '_')}_MOYENNE\"\n",
    "        air_quality_pivot = air_quality_pivot.withColumnRenamed(col_name, new_col_name)\n",
    "\n",
    "print(\"\\nStructure pivot√©e des donn√©es de qualit√© de l'air (pr√™te pour la jointure finale) :\")\n",
    "air_quality_pivot.printSchema()\n",
    "air_quality_pivot.show(5)\n",
    "\n",
    "# --- 5. Persistance sur HDFS ---\n",
    "air_quality_pivot.write.mode(\"overwrite\").parquet(AIR_QUALITY_OUTPUT_PATH)\n",
    "print(f\"‚úÖ Qualit√© de l'air pivot√©e et persist√©e sur HDFS √† : {AIR_QUALITY_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57250055-83e0-4979-bf26-e8c0ef230c29",
   "metadata": {},
   "source": [
    "Phase 9 : Jointure Finale et Persistance du Jeu de Donn√©es Complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f8a8014-a688-46a2-9996-6a3fb9a1e2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Structure du Jeu de Donn√©es Final (M√©t√©o + Qualit√© de l'Air) :\n",
      "root\n",
      " |-- GEOJOIN_ID: string (nullable = true)\n",
      " |-- DATE_OBSERVATION: string (nullable = true)\n",
      " |-- TEMP_MOYENNE_F: double (nullable = true)\n",
      " |-- PRECIPITATION_POUCES: double (nullable = true)\n",
      " |-- VITESSE_VENT_NOEUDS: double (nullable = true)\n",
      " |-- PHENOMENES: string (nullable = true)\n",
      " |-- QA_Annual_vehicle_miles_traveled_MOYENNE: double (nullable = true)\n",
      " |-- QA_Annual_vehicle_miles_traveled_cars_MOYENNE: double (nullable = true)\n",
      " |-- QA_Annual_vehicle_miles_traveled_trucks_MOYENNE: double (nullable = true)\n",
      " |-- QA_Asthma_emergency_department_visits_due_to_PM2_5_MOYENNE: double (nullable = true)\n",
      " |-- QA_Asthma_emergency_departments_visits_due_to_Ozone_MOYENNE: double (nullable = true)\n",
      " |-- QA_Asthma_hospitalizations_due_to_Ozone_MOYENNE: double (nullable = true)\n",
      " |-- QA_Boiler_Emissions-_Total_NOx_Emissions_MOYENNE: double (nullable = true)\n",
      " |-- QA_Boiler_Emissions-_Total_PM2_5_Emissions_MOYENNE: double (nullable = true)\n",
      " |-- QA_Boiler_Emissions-_Total_SO2_Emissions_MOYENNE: double (nullable = true)\n",
      " |-- QA_Cardiac_and_respiratory_deaths_due_to_Ozone_MOYENNE: double (nullable = true)\n",
      " |-- QA_Cardiovascular_hospitalizations_due_to_PM2_5_age_40+_MOYENNE: double (nullable = true)\n",
      " |-- QA_Deaths_due_to_PM2_5_MOYENNE: double (nullable = true)\n",
      " |-- QA_Fine_particles_PM_2_5_MOYENNE: double (nullable = true)\n",
      " |-- QA_Nitrogen_dioxide_NO2_MOYENNE: double (nullable = true)\n",
      " |-- QA_Outdoor_Air_Toxics_-_Benzene_MOYENNE: double (nullable = true)\n",
      " |-- QA_Outdoor_Air_Toxics_-_Formaldehyde_MOYENNE: double (nullable = true)\n",
      " |-- QA_Ozone_O3_MOYENNE: double (nullable = true)\n",
      " |-- QA_Respiratory_hospitalizations_due_to_PM2_5_age_20+_MOYENNE: double (nullable = true)\n",
      " |-- DISTANCE_STATION_KM: double (nullable = true)\n",
      " |-- ID_STATION: string (nullable = true)\n",
      " |-- LATITUDE_ZONE: double (nullable = true)\n",
      " |-- LONGITUDE_ZONE: double (nullable = true)\n",
      "\n",
      "+----------+----------------+--------------+--------------------+-------------------+----------+----------------------------------------+---------------------------------------------+-----------------------------------------------+----------------------------------------------------------+-----------------------------------------------------------+-----------------------------------------------+------------------------------------------------+--------------------------------------------------+------------------------------------------------+------------------------------------------------------+---------------------------------------------------------------+------------------------------+--------------------------------+-------------------------------+---------------------------------------+--------------------------------------------+-------------------+------------------------------------------------------------+-------------------+----------+-------------+--------------+\n",
      "|GEOJOIN_ID|DATE_OBSERVATION|TEMP_MOYENNE_F|PRECIPITATION_POUCES|VITESSE_VENT_NOEUDS|PHENOMENES|QA_Annual_vehicle_miles_traveled_MOYENNE|QA_Annual_vehicle_miles_traveled_cars_MOYENNE|QA_Annual_vehicle_miles_traveled_trucks_MOYENNE|QA_Asthma_emergency_department_visits_due_to_PM2_5_MOYENNE|QA_Asthma_emergency_departments_visits_due_to_Ozone_MOYENNE|QA_Asthma_hospitalizations_due_to_Ozone_MOYENNE|QA_Boiler_Emissions-_Total_NOx_Emissions_MOYENNE|QA_Boiler_Emissions-_Total_PM2_5_Emissions_MOYENNE|QA_Boiler_Emissions-_Total_SO2_Emissions_MOYENNE|QA_Cardiac_and_respiratory_deaths_due_to_Ozone_MOYENNE|QA_Cardiovascular_hospitalizations_due_to_PM2_5_age_40+_MOYENNE|QA_Deaths_due_to_PM2_5_MOYENNE|QA_Fine_particles_PM_2_5_MOYENNE|QA_Nitrogen_dioxide_NO2_MOYENNE|QA_Outdoor_Air_Toxics_-_Benzene_MOYENNE|QA_Outdoor_Air_Toxics_-_Formaldehyde_MOYENNE|QA_Ozone_O3_MOYENNE|QA_Respiratory_hospitalizations_due_to_PM2_5_age_20+_MOYENNE|DISTANCE_STATION_KM|ID_STATION|LATITUDE_ZONE|LONGITUDE_ZONE|\n",
      "+----------+----------------+--------------+--------------------+-------------------+----------+----------------------------------------+---------------------------------------------+-----------------------------------------------+----------------------------------------------------------+-----------------------------------------------------------+-----------------------------------------------+------------------------------------------------+--------------------------------------------------+------------------------------------------------+------------------------------------------------------+---------------------------------------------------------------+------------------------------+--------------------------------+-------------------------------+---------------------------------------+--------------------------------------------+-------------------+------------------------------------------------------------+-------------------+----------+-------------+--------------+\n",
      "+----------+----------------+--------------+--------------------+-------------------+----------+----------------------------------------+---------------------------------------------+-----------------------------------------------+----------------------------------------------------------+-----------------------------------------------------------+-----------------------------------------------+------------------------------------------------+--------------------------------------------------+------------------------------------------------+------------------------------------------------------+---------------------------------------------------------------+------------------------------+--------------------------------+-------------------------------+---------------------------------------+--------------------------------------------+-------------------+------------------------------------------------------------+-------------------+----------+-------------+--------------+\n",
      "\n",
      "\n",
      "‚úÖ Jeu de donn√©es final (M√©t√©o + Qualit√© de l'Air) cr√©√© et persist√© √† : hdfs://namenode:9000/user/mathis/datalake/nyc_final_air_weather_dataset.parquet\n",
      "Le pipeline d'Ingestion, Persistance et Traitement est termin√©. Votre jeu de donn√©es est pr√™t pour l'Insight. üìä\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# --- Configuration des Chemins (R√©utilis√©s) ---\n",
    "# Chemin des donn√©es NOAA enrichies (Couche Insight apr√®s Phase 7)\n",
    "INSIGHT_OUTPUT_PATH = \"hdfs://namenode:9000/user/mathis/datalake/noaa_gsod_nyc_enriched_clean.parquet\"\n",
    "# Chemin des donn√©es Qualit√© de l'Air pivot√©es (Couche Pivot apr√®s Phase 8)\n",
    "AIR_QUALITY_OUTPUT_PATH = \"hdfs://namenode:9000/user/mathis/datalake/nyc_air_quality_pivot.parquet\"\n",
    "# Chemin du jeu de donn√©es final\n",
    "FINAL_DATASET_PATH = \"hdfs://namenode:9000/user/mathis/datalake/nyc_final_air_weather_dataset.parquet\"\n",
    "\n",
    "# --- 1. Lecture des deux sources enrichies depuis HDFS ---\n",
    "final_insights_df = spark.read.parquet(INSIGHT_OUTPUT_PATH)\n",
    "air_quality_pivot = spark.read.parquet(AIR_QUALITY_OUTPUT_PATH)\n",
    "\n",
    "# --- 2. Jointure Finale Temporelle et G√©ographique ---\n",
    "# La jointure se fait sur deux cl√©s : l'ID de la zone et la Date de l'observation\n",
    "final_dataset = final_insights_df.join(\n",
    "    air_quality_pivot,\n",
    "    on=[\"GEOJOIN_ID\", \"DATE_OBSERVATION\"],\n",
    "    how=\"inner\" # Utilisation de 'inner' pour ne garder que les jours o√π les DEUX mesures sont pr√©sentes\n",
    ")\n",
    "\n",
    "# --- 3. S√©lection Finale et Sauvegarde ---\n",
    "# R√©organisation et s√©lection des colonnes pour le jeu de donn√©es d'analyse finale\n",
    "final_dataset = final_dataset.select(\n",
    "    \"GEOJOIN_ID\", \n",
    "    \"DATE_OBSERVATION\", \n",
    "    # M√©t√©o\n",
    "    \"TEMP_MOYENNE_F\", \n",
    "    \"PRECIPITATION_POUCES\", \n",
    "    \"VITESSE_VENT_NOEUDS\", \n",
    "    \"PHENOMENES\",\n",
    "    # Qualit√© de l'Air (toutes les colonnes commen√ßant par QA_)\n",
    "    *[col for col in final_dataset.columns if col.startswith(\"QA_\")],\n",
    "    # M√©tadonn√©es de jointure\n",
    "    F.col(\"DISTANCE_KM\").alias(\"DISTANCE_STATION_KM\"),\n",
    "    \"ID_STATION\",\n",
    "    \"LATITUDE_ZONE\", \n",
    "    \"LONGITUDE_ZONE\" \n",
    ")\n",
    "\n",
    "print(\"\\nStructure du Jeu de Donn√©es Final (M√©t√©o + Qualit√© de l'Air) :\")\n",
    "final_dataset.printSchema()\n",
    "final_dataset.show(5)\n",
    "\n",
    "# --- 4. Persistance Finale sur HDFS ---\n",
    "final_dataset.write.mode(\"overwrite\").parquet(FINAL_DATASET_PATH)\n",
    "\n",
    "print(f\"\\n‚úÖ Jeu de donn√©es final (M√©t√©o + Qualit√© de l'Air) cr√©√© et persist√© √† : {FINAL_DATASET_PATH}\")\n",
    "print(\"Le pipeline d'Ingestion, Persistance et Traitement est termin√©. Votre jeu de donn√©es est pr√™t pour l'Insight. üìä\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985c9740-407f-434f-8f3a-c2312cc3f891",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
