{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "494b1670-458c-4574-bb7d-7922f7fa98c9",
   "metadata": {},
   "source": [
    "Import des librairies nécessaires à la préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b9f4fa0-c8c6-44d4-8814-92f0179c026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ef04c-ce57-466a-88c0-7ac8fb7168cb",
   "metadata": {},
   "source": [
    "Récupération du fichier avec la correspondance id_station et coordonnées GPS pour ne garder que les fichiers des stations près de NYC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225f0e44-3643-436f-9b70-d3f01791295c",
   "metadata": {},
   "source": [
    "Phase 1 : Initialisation de l'Environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d6045c9-0264-4370-95a1-2268f3da9fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session Spark initialisée avec les configurations de robustesse.\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "# --- 1. Configuration de Robustesse et Initialisation Spark ---\n",
    "# Augmentation des timeouts et allocation de mémoire stricte pour éviter les crashs JVM\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataLake_NOAA_NYC_Prep\") \\\n",
    "    .config(\"spark.executor.memory\", \"3g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.rpc.askTimeout\", \"800s\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# --- 2. Définition des Paramètres Géographiques et HDFS ---\n",
    "# Boîte englobante de la région de NYC\n",
    "MIN_LAT, MAX_LAT = 40.0, 41.5\n",
    "MIN_LON, MAX_LON = -75.0, -73.0\n",
    "\n",
    "# Chemin HDFS BRUT (Corrigé avec l'hôte et le port du namenode)\n",
    "RAW_OUTPUT_PATH = \"hdfs://namenode:9000/user/mathis/datalake/noaa_gsod_nyc_raw_2005_2023.parquet\"\n",
    "\n",
    "# Plage d'années\n",
    "START_YEAR = 2005\n",
    "END_YEAR = 2023\n",
    "\n",
    "print(\"Session Spark initialisée avec les configurations de robustesse.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249f1c05-11fd-4a1a-8655-ce2a03ca0fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Phase 2 : Métadonnées et Identification des Stations NOAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a6ad18d-ae5c-4807-95eb-3b54176a858a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 93 Stations NOAA pertinentes trouvées près de New York.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Téléchargement des Métadonnées des Stations (via Pandas car petit fichier) ---\n",
    "stations_url = \"https://www.ncei.noaa.gov/pub/data/noaa/isd-history.csv\"\n",
    "pdf_stations = pd.read_csv(stations_url,\n",
    "                         dtype={'USAF': str, 'WBAN': str})\n",
    "\n",
    "pdf_stations['STN_ID'] = pdf_stations['USAF'].str.strip() + pdf_stations['WBAN'].str.strip()\n",
    "pdf_stations = pdf_stations.rename(columns={'LAT': 'LATITUDE', 'LON': 'LONGITUDE'})\n",
    "pdf_stations = pdf_stations.dropna(subset=['LATITUDE', 'LONGITUDE', 'STATION NAME'])\n",
    "spark_stations_df = spark.createDataFrame(pdf_stations)\n",
    "\n",
    "# --- 2. Filtrage Géographique ---\n",
    "nyc_stations_spark = spark_stations_df.filter(\n",
    "    (F.col('LATITUDE') >= MIN_LAT) & (F.col('LATITUDE') <= MAX_LAT) &\n",
    "    (F.col('LONGITUDE') >= MIN_LON) & (F.col('LONGITUDE') <= MAX_LON)\n",
    ")\n",
    "\n",
    "# Récupération de la liste des IDs pertinents (pour filtrage par nom de fichier)\n",
    "relevant_station_ids = [row.STN_ID for row in nyc_stations_spark.select(\"STN_ID\").collect()]\n",
    "\n",
    "print(f\"\\n✅ {nyc_stations_spark.count()} Stations NOAA pertinentes trouvées près de New York.\")\n",
    "# Gardons ce DataFrame pour la jointure des coordonnées plus tard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21de6ad-c870-438e-bd32-0eb6f005656f",
   "metadata": {},
   "source": [
    "Phase 2 bis : Téléchargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd338ec-5fac-406b-b377-aed46fa0e614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Démarrage du téléchargement pour 93 stations de 2005 à 2023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Téléchargement des fichiers GSOD:   8%|▊         | 137/1767 [05:24<36:26,  1.34s/it]  "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm # Importation de tqdm pour une barre de progression\n",
    "\n",
    "# --- Paramètres de Configuration ---\n",
    "BASE_URL = \"https://www.ncei.noaa.gov/data/global-summary-of-the-day/access\"\n",
    "LOCAL_BASE_DIR = \"/home/jovyan/work/data/noaa_gsod\"\n",
    "START_YEAR = 2005\n",
    "END_YEAR = 2023\n",
    "\n",
    "# --- IDs des stations à télécharger ---\n",
    "# NOTE: Cette liste doit être remplie avec le résultat de votre Phase 2 !\n",
    "# Exemple de données de test si la Phase 2 n'est pas exécutée :\n",
    "# relevant_station_ids = [\"72503094728\", \"72503014732\"] \n",
    "\n",
    "# Si vous exécutez ce bloc après la Phase 2, assurez-vous que la liste est disponible.\n",
    "if 'relevant_station_ids' not in locals():\n",
    "    print(\"⚠️ ATTENTION: La liste 'relevant_station_ids' n'est pas définie. Veuillez exécuter la Phase 2 en premier.\")\n",
    "    # On sort du script si la liste n'est pas disponible pour éviter de télécharger inutilement\n",
    "    exit()\n",
    "\n",
    "# Démarrage du processus de téléchargement\n",
    "print(f\"Démarrage du téléchargement pour {len(relevant_station_ids)} stations de {START_YEAR} à {END_YEAR}.\")\n",
    "\n",
    "total_files = len(relevant_station_ids) * (END_YEAR - START_YEAR + 1)\n",
    "downloaded_count = 0\n",
    "\n",
    "# Utilisation de tqdm pour la barre de progression\n",
    "with tqdm(total=total_files, desc=\"Téléchargement des fichiers GSOD\") as pbar:\n",
    "    \n",
    "    for year in range(START_YEAR, END_YEAR + 1):\n",
    "        year_dir = os.path.join(LOCAL_BASE_DIR, str(year))\n",
    "        \n",
    "        # Crée le répertoire de l'année s'il n'existe pas\n",
    "        os.makedirs(year_dir, exist_ok=True)\n",
    "        \n",
    "        for station_id in relevant_station_ids:\n",
    "            file_name = f\"{station_id}.csv\"\n",
    "            local_path = os.path.join(year_dir, file_name)\n",
    "            remote_url = f\"{BASE_URL}/{year}/{file_name}\"\n",
    "\n",
    "            # Vérifie si le fichier existe déjà pour éviter de le re-télécharger\n",
    "            if os.path.exists(local_path):\n",
    "                # print(f\"Fichier existant: {local_path}. Ignoré.\")\n",
    "                pbar.update(1)\n",
    "                downloaded_count += 1\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Requête HTTP GET pour télécharger le fichier\n",
    "                response = requests.get(remote_url, timeout=10)\n",
    "                response.raise_for_status() # Lève une exception si le statut HTTP est 4xx ou 5xx\n",
    "                \n",
    "                # Écrit le contenu dans le fichier local\n",
    "                with open(local_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                \n",
    "                downloaded_count += 1\n",
    "                pbar.update(1)\n",
    "                # Pause pour être poli avec le serveur NOAA\n",
    "                time.sleep(0.1) \n",
    "                \n",
    "            except requests.exceptions.HTTPError as errh:\n",
    "                # Fichier 404/Not Found (la station n'a pas forcément de données pour cette année)\n",
    "                if response.status_code == 404:\n",
    "                    pass # On ignore simplement ce fichier manquant\n",
    "                else:\n",
    "                    print(f\"\\n❌ Erreur HTTP pour {remote_url}: {errh}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"\\n❌ Erreur de Connexion/Timeout pour {remote_url}: {e}\")\n",
    "\n",
    "print(f\"\\n✅ Téléchargement terminé. {downloaded_count} fichiers GSOD traités (téléchargés ou existants).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bbb709-9985-4754-8f81-3724928ef9cb",
   "metadata": {},
   "source": [
    "Phase 3 : Ingestion Ciblée et Persistance (Couche Raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a98c2825-e1c6-450b-a5dd-40d475bcfa78",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/data/noaa_gsod/2005/72055399999.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 32\u001b[0m\n\u001b[1;32m     12\u001b[0m gsod_schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[1;32m     13\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTATION\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     14\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFRSHHT\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     29\u001b[0m ])\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Lecture distribuée des données GSOD (seulement les fichiers ciblés)\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m all_gsod_data \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgsod_data_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgsod_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     37\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Renommage de la colonne ID\u001b[39;00m\n\u001b[1;32m     40\u001b[0m nyc_gsod_data \u001b[38;5;241m=\u001b[39m all_gsod_data\u001b[38;5;241m.\u001b[39mwithColumnRenamed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTATION\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID_STATION\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/data/noaa_gsod/2005/72055399999.csv."
     ]
    }
   ],
   "source": [
    "# --- Configuration des Chemins ---\n",
    "LOCAL_BASE_DIR = \"/home/jovyan/work/data/noaa_gsod\" \n",
    "RAW_OUTPUT_PATH = \"hdfs://namenode:9000/user/mathis/datalake/noaa_gsod_nyc_raw_2005_2023.parquet\"\n",
    "START_YEAR = 2005\n",
    "END_YEAR = 2023\n",
    "\n",
    "# --- 1. Définition des Chemins Ciblés ---\n",
    "# Nous construisons la liste des chemins vers les fichiers des stations pertinentes\n",
    "targeted_paths = []\n",
    "for year in range(START_YEAR, END_YEAR + 1):\n",
    "    for station_id in relevant_station_ids:\n",
    "        # Chemin absolu corrigé : /home/jovyan/work/data/noaa_gsod/2005/XXXXX.csv\n",
    "        targeted_paths.append(f\"{LOCAL_BASE_DIR}/{year}/{station_id}.csv\")\n",
    "\n",
    "gsod_data_paths = targeted_paths\n",
    "\n",
    "# --- 2. Schéma et Lecture ---\n",
    "gsod_schema = StructType([\n",
    "    StructField(\"STATION\", StringType(), True),\n",
    "    StructField(\"DATE\", StringType(), True),\n",
    "    # Inclure le reste du schéma complet... (omission ici pour la concision)\n",
    "    StructField(\"FRSHHT\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Lecture distribuée des données GSOD (seulement les fichiers ciblés)\n",
    "all_gsod_data = spark.read.csv(\n",
    "    gsod_data_paths,\n",
    "    header=True,\n",
    "    schema=gsod_schema,\n",
    "    sep=','\n",
    ")\n",
    "\n",
    "# Renommage de la colonne ID\n",
    "nyc_gsod_data = all_gsod_data.withColumnRenamed(\"STATION\", \"ID_STATION\")\n",
    "\n",
    "\n",
    "# --- 3. Persistance de la Couche Brute sur HDFS ---\n",
    "print(f\"\\nSauvegarde de la copie BRUTE filtrée (2005-2023) dans : {RAW_OUTPUT_PATH}...\")\n",
    "# Cette étape transfère les données du disque local du conteneur vers HDFS\n",
    "nyc_gsod_data.write.mode(\"overwrite\").parquet(RAW_OUTPUT_PATH)\n",
    "print(\"✅ Copie brute sauvegardée sur HDFS. Le traitement peut se poursuivre.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "469c5cb3-ce66-4ed8-80a9-f0a9ab6aebd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 33620)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o338.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 41\u001b[0m\n\u001b[1;32m     20\u001b[0m gsod_schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[1;32m     21\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTATION\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     22\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFRSHHT\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     37\u001b[0m ])\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Lecture distribuée des données GSOD (UNiquement 2005-2023)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Notez l'utilisation de `gsod_data_paths` (la liste)\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m all_gsod_data \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgsod_data_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgsod_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     46\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Renommage de la colonne ID et Filtrage du gros DataFrame\u001b[39;00m\n\u001b[1;32m     49\u001b[0m gsod_data_filtered \u001b[38;5;241m=\u001b[39m all_gsod_data\u001b[38;5;241m.\u001b[39mwithColumnRenamed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTATION\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID_STATION\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o338.csv"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# --- Configuration HDFS ---\n",
    "# Chemin où les données BRUTES FILTRÉES seront sauvegardées (votre couche Raw/Persistance)\n",
    "RAW_OUTPUT_PATH = \"hdfs://namenode:9000/user/mathis/datalake/noaa_gsod_nyc_raw_2005_2023.parquet\"\n",
    "\n",
    "# --- 1. Définition de la Plage de Chemins (LA CORRECTION) ---\n",
    "start_year = 2005\n",
    "end_year = 2005\n",
    "\n",
    "# Création d'une liste de chemins cibles :\n",
    "# Exemple: ['/data/noaa_gsod/2005/*.csv', '/data/noaa_gsod/2006/*.csv', ...]\n",
    "year_paths = [f\"../data/noaa_gsod/{year}/*.csv\" for year in range(start_year, end_year + 1)]\n",
    "\n",
    "# Le chemin de lecture devient la liste des chemins d'années\n",
    "gsod_data_paths = year_paths\n",
    "\n",
    "# Schéma COMPLÉTÉ des colonnes\n",
    "gsod_schema = StructType([\n",
    "    StructField(\"STATION\", StringType(), True),\n",
    "    StructField(\"DATE\", StringType(), True),\n",
    "    StructField(\"LATITUDE\", DoubleType(), True), \n",
    "    StructField(\"LONGITUDE\", DoubleType(), True),\n",
    "    StructField(\"ELEVATION\", DoubleType(), True),\n",
    "    StructField(\"NAME\", StringType(), True),\n",
    "    StructField(\"TEMP\", DoubleType(), True),\n",
    "    StructField(\"TEMP_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"DEWP\", DoubleType(), True),\n",
    "    StructField(\"PRCP\", DoubleType(), True),\n",
    "    StructField(\"MIN\", DoubleType(), True),\n",
    "    StructField(\"MAX\", DoubleType(), True),\n",
    "    StructField(\"WDSP\", DoubleType(), True),\n",
    "    StructField(\"VISIB\", DoubleType(), True),\n",
    "    StructField(\"SLP\", DoubleType(), True),\n",
    "    StructField(\"FRSHHT\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Lecture distribuée des données GSOD (UNiquement 2005-2023)\n",
    "# Notez l'utilisation de `gsod_data_paths` (la liste)\n",
    "all_gsod_data = spark.read.csv(\n",
    "    gsod_data_paths,\n",
    "    header=True,\n",
    "    schema=gsod_schema,\n",
    "    sep=','\n",
    ")\n",
    "\n",
    "# Renommage de la colonne ID et Filtrage du gros DataFrame\n",
    "gsod_data_filtered = all_gsod_data.withColumnRenamed(\"STATION\", \"ID_STATION\")\n",
    "nyc_gsod_data = gsod_data_filtered.filter(F.col(\"ID_STATION\").isin(relevant_station_ids))\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# NOUVELLE ÉTAPE : Persistance de la copie Brute filtrée sur HDFS\n",
    "# Le format Parquet est compressé et optimisé pour le Big Data\n",
    "print(f\"Sauvegarde de la copie BRUTE filtrée (2005-2023) dans : {RAW_OUTPUT_PATH}...\")\n",
    "nyc_gsod_data.write.mode(\"overwrite\").parquet(RAW_OUTPUT_PATH)\n",
    "print(\"✅ Copie brute sauvegardée sur HDFS. Le traitement peut se poursuivre à partir de ce point.\")\n",
    "# ----------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d6402b-0c18-47e6-ad86-79d0c4c1af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture du fichier Parquet depuis HDFS pour la couche de Traitement/Nettoyage (ETL)\n",
    "data_for_cleaning = spark.read.parquet(RAW_OUTPUT_PATH)\n",
    "# --- 2. Nettoyage et Renommage des Colonnes (en Français) ---\n",
    "\n",
    "renaming_map = {\n",
    "    \"DATE\": \"DATE_OBSERVATION\",\n",
    "    \"TEMP\": \"TEMP_MOYENNE_F\",\n",
    "    \"DEWP\": \"POINT_ROSEE_F\",\n",
    "    \"PRCP\": \"PRECIPITATION_POUCES\",\n",
    "    \"MIN\": \"TEMP_MIN_F\",\n",
    "    \"MAX\": \"TEMP_MAX_F\",\n",
    "    \"WDSP\": \"VITESSE_VENT_NOEUDS\",\n",
    "    \"VISIB\": \"VISIBILITE_MILLES\",\n",
    "    \"SLP\": \"PRESSION_ATM_MER\",\n",
    "    \"FRSHHT\": \"PHENOMENES\"\n",
    "}\n",
    "\n",
    "gsod_renamed_df = data_for_cleaning\n",
    "for old_name, new_name in renaming_map.items():\n",
    "    if old_name in gsod_renamed_df.columns:\n",
    "        gsod_renamed_df = gsod_renamed_df.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "\n",
    "# --- 3. Jointure des Coordonnées (si manquantes dans les CSV GSOD) ---\n",
    "# On utilise la jointure pour s'assurer que les coordonnées des stations sont présentes\n",
    "\n",
    "clean_final_gsod_df = gsod_renamed_df.join(\n",
    "    nyc_stations_spark.select(\n",
    "        'STN_ID', \n",
    "        F.col('LATITUDE').alias('LATITUDE_STATION'), \n",
    "        F.col('LONGITUDE').alias('LONGITUDE_STATION')\n",
    "    ), \n",
    "    on=[F.col(\"ID_STATION\") == F.col(\"STN_ID\")],\n",
    "    how='left'\n",
    ").drop(\"STN_ID\").drop(\"LATITUDE\").drop(\"LONGITUDE\") # Supprime les colonnes de l'ancien CSV si elles existent\n",
    "\n",
    "\n",
    "print(\"\\nDonnées NOAA nettoyées et prêtes pour la jointure spatiale.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50039ef3-7942-45b3-8e14-7afed3b69d33",
   "metadata": {},
   "source": [
    "Phase 4 : Préparation des Coordonnées des Zones de Qualité de l'Air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aa51dc-fa33-4173-be90-fbce7fe02ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Téléchargement et Extraction du GeoJSON ---\n",
    "geojson_url = \"https://raw.githubusercontent.com/nycehs/NYC_geography/master/UHF42.geo.json\"\n",
    "\n",
    "response = requests.get(geojson_url)\n",
    "geo_data_raw = response.json()\n",
    "\n",
    "geo_records = []\n",
    "for feature in geo_data_raw['features']:\n",
    "    properties = feature['properties']\n",
    "    geometry = feature['geometry']\n",
    "    coords = geometry['coordinates']\n",
    "    \n",
    "    try:\n",
    "        # Extraction du premier point du polygone/multipolygone comme centroïde approximatif\n",
    "        if geometry['type'] == 'Polygon':\n",
    "            lon = coords[0][0][0]\n",
    "            lat = coords[0][0][1]\n",
    "        elif geometry['type'] == 'MultiPolygon':\n",
    "            lon = coords[0][0][0][0]\n",
    "            lat = coords[0][0][0][1]\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        geo_records.append({\n",
    "            \"GEOJOIN_ID\": properties['UHF42'],\n",
    "            \"LATITUDE_ZONE\": lat,\n",
    "            \"LONGITUDE_ZONE\": lon\n",
    "        })\n",
    "    except (IndexError, TypeError):\n",
    "        continue\n",
    "\n",
    "# --- 2. Création du DataFrame Spark des Coordonnées des Zones ---\n",
    "geo_schema = StructType([\n",
    "    StructField(\"GEOJOIN_ID\", StringType(), False),\n",
    "    StructField(\"LATITUDE_ZONE\", DoubleType(), True),\n",
    "    StructField(\"LONGITUDE_ZONE\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "geo_df = spark.createDataFrame(geo_records, schema=geo_schema)\n",
    "\n",
    "print(f\"\\n{geo_df.count()} zones de qualité de l'air (GeoJoin ID) préparées avec leurs coordonnées.\")\n",
    "geo_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49c6359-22e0-4230-bbcf-160d35d30d6e",
   "metadata": {},
   "source": [
    "Phase 5 : Calcul de la Distance et Voisin le Plus Proche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53471d4c-5d53-4e8c-8bfc-c5e04cc59eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constante du rayon de la Terre en kilomètres (km)\n",
    "R = 6371.0\n",
    "\n",
    "# Formule de la Haversine\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calcule la distance Haversine entre deux points (lat, lon)\n",
    "    Retourne la distance en kilomètres (km)\n",
    "    \"\"\"\n",
    "    # Conversion de degrés en radians\n",
    "    lon1_rad, lat1_rad, lon2_rad, lat2_rad = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1_rad) * cos(lat2_rad) * sin(dlon / 2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    \n",
    "    return R * c\n",
    "\n",
    "# Enregistrement de la fonction comme une UDF (User Defined Function) pour Spark\n",
    "haversine_udf = F.udf(haversine, DoubleType())\n",
    "\n",
    "print(\"\\nFonction Haversine UDF créée pour le calcul de distance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86a1c72-6491-4f7c-87fd-9de2989b9f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Préparation pour la Jointure Cartésienne (Cross Join) ---\n",
    "# On sélectionne les informations de position uniques de chaque station météo\n",
    "stations_coords_df = clean_final_gsod_df.select(\n",
    "    \"ID_STATION\", \"LATITUDE_STATION\", \"LONGITUDE_STATION\"\n",
    ").distinct()\n",
    "\n",
    "# --- 2. Jointure Cartésienne ---\n",
    "# Jointure de toutes les zones de qualité de l'air avec toutes les stations météo.\n",
    "# Cette étape crée toutes les paires possibles (Zone A - Station 1, Zone A - Station 2, ...)\n",
    "# ATTENTION: Le Cross Join doit rester petit (n_stations x n_zones) pour ne pas saturer la RAM.\n",
    "cross_joined_df = geo_df.crossJoin(stations_coords_df)\n",
    "\n",
    "\n",
    "# --- 3. Calcul de la Distance pour chaque paire ---\n",
    "# On applique la fonction Haversine\n",
    "distance_df = cross_joined_df.withColumn(\n",
    "    \"DISTANCE_KM\",\n",
    "    haversine_udf(\n",
    "        F.col(\"LONGITUDE_ZONE\"), \n",
    "        F.col(\"LATITUDE_ZONE\"), \n",
    "        F.col(\"LONGITUDE_STATION\"), \n",
    "        F.col(\"LATITUDE_STATION\")\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# --- 4. Identification du Voisin le Plus Proche ---\n",
    "# On utilise une Window Function pour trouver la distance minimale pour chaque zone (GeoJoin ID)\n",
    "window_spec = Window.partitionBy(\"GEOJOIN_ID\").orderBy(F.col(\"DISTANCE_KM\"))\n",
    "\n",
    "nearest_station_df = distance_df.withColumn(\n",
    "    \"rank\", \n",
    "    F.rank().over(window_spec)\n",
    ").filter(F.col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "\n",
    "print(\"\\n✅ Voisin le plus proche trouvé pour chaque zone de qualité de l'air :\")\n",
    "nearest_station_df.select(\n",
    "    \"GEOJOIN_ID\", \n",
    "    \"ID_STATION\", \n",
    "    \"DISTANCE_KM\", \n",
    "    \"LATITUDE_ZONE\", \n",
    "    \"LATITUDE_STATION\"\n",
    ").show(5, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
