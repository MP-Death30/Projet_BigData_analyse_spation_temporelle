{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0bef17-e0d3-4a2e-a48a-f87ab105bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BibliothÃ¨ques standard\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "# BibliothÃ¨ques tierces\n",
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from IPython.display import display\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import to_date, col, lit, substring, regexp_replace\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac6f5db-9076-4f36-9499-56c0fc9306c7",
   "metadata": {},
   "source": [
    "# PrÃ©paration des donnÃ©es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aca566-92f4-464c-9c85-95fd802477d1",
   "metadata": {},
   "source": [
    "## Phase 1 : Initialisation de l'Environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408cc07a-fda5-4d7b-9ef3-b6b641a1014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Configuration et Initialisation de Spark ---\n",
    "# Augmentation des timeouts et allocation de mÃ©moire stricte pour Ã©viter les crashs JVM\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataLake_NOAA_NYC_Prep\") \\\n",
    "    .config(\"spark.executor.memory\", \"3g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.rpc.askTimeout\", \"800s\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# --- 2. DÃ©finition des ParamÃ¨tres GÃ©ographiques et HDFS ---\n",
    "# BoÃ®te englobante de la rÃ©gion de NYC afin de restreindre l'import des donnÃ©es NOAA\n",
    "MIN_LAT, MAX_LAT = 40.0, 41.5\n",
    "MIN_LON, MAX_LON = -75.0, -73.0\n",
    "\n",
    "# Chemin HDFS BRUT\n",
    "RAW_OUTPUT_PATH = \"hdfs://namenode:9000/user/mathis/datalake/noaa_gsod_nyc_raw_2005_2023.parquet\"\n",
    "\n",
    "# Plage d'annÃ©es\n",
    "START_YEAR = 2005\n",
    "END_YEAR = 2023\n",
    "\n",
    "print(\"âœ… Session Spark configurÃ©e et initialisÃ©e.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b968a45-1bcf-4648-9174-5232e8494a1a",
   "metadata": {},
   "source": [
    "## Phase 2 : MÃ©tadonnÃ©es et Identification des Stations NOAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5465fda2-fbcf-433b-b657-abf861fbdb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. TÃ©lÃ©chargement des MÃ©tadonnÃ©es des Stations ---\n",
    "stations_url = \"https://www.ncei.noaa.gov/pub/data/noaa/isd-history.csv\"\n",
    "pdf_stations = pd.read_csv(stations_url,\n",
    "                         dtype={'USAF': str, 'WBAN': str})\n",
    "\n",
    "pdf_stations['STN_ID'] = pdf_stations['USAF'].str.strip() + pdf_stations['WBAN'].str.strip()\n",
    "pdf_stations = pdf_stations.rename(columns={'LAT': 'LATITUDE', 'LON': 'LONGITUDE'})\n",
    "pdf_stations = pdf_stations.dropna(subset=['LATITUDE', 'LONGITUDE', 'STATION NAME'])\n",
    "spark_stations_df = spark.createDataFrame(pdf_stations)\n",
    "\n",
    "# --- 2. Filtrage GÃ©ographique ---\n",
    "nyc_stations_spark = spark_stations_df.filter(\n",
    "    (F.col('LATITUDE') >= MIN_LAT) & (F.col('LATITUDE') <= MAX_LAT) &\n",
    "    (F.col('LONGITUDE') >= MIN_LON) & (F.col('LONGITUDE') <= MAX_LON)\n",
    ")\n",
    "\n",
    "# RÃ©cupÃ©ration de la liste des IDs pertinents (pour filtrage par nom de fichier)\n",
    "relevant_station_ids = [row.STN_ID for row in nyc_stations_spark.select(\"STN_ID\").collect()]\n",
    "\n",
    "print(f\"\\nâœ… {nyc_stations_spark.count()} Stations NOAA pertinentes trouvÃ©es prÃ¨s de New York.\")\n",
    "# Gardons ce DataFrame pour la jointure des coordonnÃ©es plus tard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68963a2-8016-4469-9211-9cfbf0794c01",
   "metadata": {},
   "source": [
    "## Phase 2 bis : TÃ©lÃ©chargement des donnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbad7f3a-e433-4507-8618-e729106a53a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://www.ncei.noaa.gov/data/global-summary-of-the-day/access\"\n",
    "LOCAL_BASE_DIR = \"/home/jovyan/work/data/noaa_gsod\"\n",
    "START_YEAR = 2005\n",
    "END_YEAR = 2023\n",
    "\n",
    "# --- IDs des stations Ã  tÃ©lÃ©charger ---\n",
    "if 'relevant_station_ids' not in locals():\n",
    "    print(\"âš ï¸ ATTENTION: La liste 'relevant_station_ids' n'est pas dÃ©finie. Veuillez exÃ©cuter la Phase 2 en premier.\")\n",
    "    exit()\n",
    "\n",
    "# DÃ©marrage du processus\n",
    "print(f\"DÃ©marrage du tÃ©lÃ©chargement pour {len(relevant_station_ids)} stations de {START_YEAR} Ã  {END_YEAR}.\")\n",
    "\n",
    "downloaded_count = 0\n",
    "\n",
    "# --- Boucle principale (sans barre de progression) ---\n",
    "for year in range(START_YEAR, END_YEAR + 1):\n",
    "    year_dir = os.path.join(LOCAL_BASE_DIR, str(year))\n",
    "    \n",
    "    # Petit print pour savoir oÃ¹ on en est (optionnel, mais utile sans barre de progression)\n",
    "    print(f\"Traitement de l'annÃ©e : {year}...\")\n",
    "\n",
    "    # CrÃ©e le rÃ©pertoire de l'annÃ©e s'il n'existe pas\n",
    "    os.makedirs(year_dir, exist_ok=True)\n",
    "\n",
    "    for station_id in relevant_station_ids:\n",
    "        file_name = f\"{station_id}.csv\"\n",
    "        local_path = os.path.join(year_dir, file_name)\n",
    "        remote_url = f\"{BASE_URL}/{year}/{file_name}\"\n",
    "\n",
    "        # VÃ©rifie si le fichier existe dÃ©jÃ \n",
    "        if os.path.exists(local_path):\n",
    "            downloaded_count += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # RequÃªte HTTP GET\n",
    "            response = requests.get(remote_url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Ã‰crit le contenu dans le fichier local\n",
    "            with open(local_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "            downloaded_count += 1\n",
    "            \n",
    "            # Pause pour Ãªtre poli avec le serveur NOAA\n",
    "            time.sleep(0.1) \n",
    "\n",
    "        except requests.exceptions.HTTPError as errh:\n",
    "            # Fichier 404/Not Found\n",
    "            if response.status_code == 404:\n",
    "                pass \n",
    "            else:\n",
    "                print(f\"\\nâŒ Erreur HTTP pour {remote_url}: {errh}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"\\nâŒ Erreur de Connexion/Timeout pour {remote_url}: {e}\")\n",
    "\n",
    "print(f\"\\nâœ… TÃ©lÃ©chargement terminÃ©. {downloaded_count} fichiers GSOD traitÃ©s (tÃ©lÃ©chargÃ©s ou existants).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ba3823-17a9-4fba-bc32-51894644af18",
   "metadata": {},
   "source": [
    "## Phase 3 : Ingestion CiblÃ©e et Persistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1aaaca-7e8e-4e8f-93ce-32027b3f8c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration des Chemins ---\n",
    "LOCAL_BASE_DIR = \"/home/jovyan/work/data/noaa_gsod\" \n",
    "RAW_OUTPUT_PATH = \"hdfs://namenode:9000/user/mathis/datalake/noaa_gsod_nyc_raw_2005_2023.parquet\"\n",
    "START_YEAR = 2005\n",
    "END_YEAR = 2023\n",
    "\n",
    "# --- 1. DÃ©finition des Chemins CiblÃ©s ---\n",
    "# Nous recrÃ©ons la liste, mais cette fois en utilisant 'glob' ou une vÃ©rification OS\n",
    "# pour ne pas inclure les chemins qui n'existent pas.\n",
    "\n",
    "existing_targeted_paths = []\n",
    "for year in range(START_YEAR, END_YEAR + 1):\n",
    "    for station_id in relevant_station_ids:\n",
    "        # Chemin absolu corrigÃ© : /home/jovyan/work/data/noaa_gsod/2005/XXXXX.csv\n",
    "        path = f\"{LOCAL_BASE_DIR}/{year}/{station_id}.csv\"\n",
    "        \n",
    "        # VÃ©rifie si le fichier existe vraiment avant de l'ajouter Ã  la liste de lecture de Spark\n",
    "        if os.path.exists(path):\n",
    "            existing_targeted_paths.append(path)\n",
    "\n",
    "# Si aucun chemin n'existe, nous aurons une erreur, mais au moins nous savons pourquoi.\n",
    "if not existing_targeted_paths:\n",
    "    raise FileNotFoundError(\"Aucun fichier GSOD cible n'a Ã©tÃ© trouvÃ© dans le rÃ©pertoire local.\")\n",
    "\n",
    "gsod_data_paths = existing_targeted_paths\n",
    "print(f\"Total de {len(gsod_data_paths)} fichiers existants seront lus par Spark.\")\n",
    "\n",
    "# --- 2. SchÃ©ma et Lecture ---\n",
    "gsod_schema = StructType([\n",
    "    StructField(\"STATION\", StringType(), True),\n",
    "    StructField(\"DATE\", StringType(), True),\n",
    "    StructField(\"LATITUDE\", DoubleType(), True), \n",
    "    StructField(\"LONGITUDE\", DoubleType(), True),\n",
    "    StructField(\"ELEVATION\", DoubleType(), True),\n",
    "    StructField(\"NAME\", StringType(), True),\n",
    "    StructField(\"TEMP\", DoubleType(), True),\n",
    "    StructField(\"TEMP_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"DEWP\", DoubleType(), True),\n",
    "    StructField(\"DEWP_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"SLP\", DoubleType(), True),\n",
    "    StructField(\"SLP_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"STP\", DoubleType(), True),\n",
    "    StructField(\"STP_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"VISIB\", DoubleType(), True),\n",
    "    StructField(\"VISIB_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"WDSP\", DoubleType(), True),\n",
    "    StructField(\"WDSP_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"MXSPD\", DoubleType(), True),\n",
    "    StructField(\"GUST\", DoubleType(), True),\n",
    "    StructField(\"MAX\", DoubleType(), True),\n",
    "    StructField(\"MAX_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"MIN\", DoubleType(), True),\n",
    "    StructField(\"MIN_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"PRCP\", DoubleType(), True),\n",
    "    StructField(\"PRCP_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"SNDP\", DoubleType(), True),\n",
    "    StructField(\"FRSHHT\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Lecture distribuÃ©e des donnÃ©es GSOD (seulement les fichiers ciblÃ©s)\n",
    "all_gsod_data = spark.read.csv(\n",
    "    gsod_data_paths,\n",
    "    header=True,\n",
    "    schema=gsod_schema,\n",
    "    sep=','\n",
    ")\n",
    "\n",
    "# Renommage de la colonne ID\n",
    "nyc_gsod_data = all_gsod_data.withColumnRenamed(\"STATION\", \"ID_STATION\")\n",
    "\n",
    "\n",
    "# --- 3. Persistance de la Couche Brute sur HDFS ---\n",
    "print(f\"\\nSauvegarde de la copie BRUTE filtrÃ©e (2005-2023) dans : {RAW_OUTPUT_PATH}...\")\n",
    "# Cette Ã©tape transfÃ¨re les donnÃ©es du disque local du conteneur vers HDFS\n",
    "nyc_gsod_data.write.mode(\"overwrite\").parquet(RAW_OUTPUT_PATH)\n",
    "print(\"âœ… Copie brute sauvegardÃ©e sur HDFS. Le traitement peut se poursuivre.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf6aee1-d259-469a-ac59-2bbf8b3e3ed1",
   "metadata": {},
   "source": [
    "## Phase 4 : TÃ©lÃ©chargement et Nettoyage du JSON Socrata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8fa00f-a310-42a8-935e-4da50409c6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration des Chemins ---\n",
    "LOCAL_BASE_DIR = \"/home/jovyan/work/data/air_quality\"\n",
    "LOCAL_JSON_PATH = os.path.join(LOCAL_BASE_DIR, \"nyc_air_quality_raw.json\")\n",
    "AIR_QUALITY_URL = \"https://data.cityofnewyork.us/api/views/c3uy-2p5r/rows.json?accessType=DOWNLOAD\"\n",
    "\n",
    "# CrÃ©e le rÃ©pertoire local si nÃ©cessaire\n",
    "os.makedirs(LOCAL_BASE_DIR, exist_ok=True)\n",
    "\n",
    "# --- 1. TÃ©lÃ©chargement et Nettoyage de la structure JSON Socrata ---\n",
    "print(f\"â¬‡ï¸ TÃ©lÃ©chargement du JSON Socrata depuis l'API de NYC...\")\n",
    "try:\n",
    "    response = requests.get(AIR_QUALITY_URL, timeout=300) # Timeout de 5 minutes\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    \n",
    "    # La clÃ© 'data' contient le tableau des enregistrements bruts que Spark doit lire.\n",
    "    raw_records = data.get('data', [])\n",
    "\n",
    "    if not raw_records:\n",
    "        print(\"âŒ Erreur : La clÃ© 'data' est vide dans le JSON tÃ©lÃ©chargÃ©. ArrÃªt du processus.\")\n",
    "        exit()\n",
    "    \n",
    "    # Ã‰criture du tableau de donnÃ©es brutes SEULEMENT dans le nouveau fichier JSON.\n",
    "    # Ceci est essentiel pour que le RDD/toDF fonctionne correctement.\n",
    "    with open(LOCAL_JSON_PATH, 'w') as f:\n",
    "        json.dump(raw_records, f)\n",
    "\n",
    "    print(f\"âœ… Fichier JSON brut sauvegardÃ© et nettoyÃ© structurellement Ã  : {LOCAL_JSON_PATH}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erreur lors du tÃ©lÃ©chargement/nettoyage : {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbcdc9e-f279-4229-bfd7-79f1e1ea8fa9",
   "metadata": {},
   "source": [
    "## Phase 5 - Transformation des donnÃ©es et CrÃ©ation du Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b611b3e-eeeb-4eb5-80c6-b3e04f2e3c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”„ DÃ©marrage de l'ETL Spark...\")\n",
    "\n",
    "# 1. Initialisation Spark\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    spark = SparkSession.builder.appName(\"OneCode_ETL\").getOrCreate()\n",
    "\n",
    "# 2. Configuration des chemins\n",
    "GEOJSON_URL = \"https://raw.githubusercontent.com/nycehs/NYC_geography/master/UHF42.geo.json\"\n",
    "AIR_QUALITY_PATH = \"/home/jovyan/work/data/air_quality/nyc_air_quality_raw.json\" \n",
    "WEATHER_PATH = \"hdfs://namenode:9000/user/mathis/datalake/noaa_gsod_nyc_raw_2005_2023.parquet\"\n",
    "\n",
    "# --- A. Traitement GÃ©ographique (GeoJSON) ---\n",
    "print(\"   ðŸ“ Traitement du GeoJSON...\")\n",
    "gdf_quartiers = gpd.read_file(GEOJSON_URL)\n",
    "\n",
    "# Correction GÃ©omÃ©trique : On projette en EPSG:2263 (NYC Feet) pour calculer le centre, puis on revient en Lat/Lon\n",
    "# Cela Ã©vite le warning et donne un centre plus prÃ©cis.\n",
    "gdf_quartiers = gdf_quartiers.to_crs(epsg=2263) \n",
    "gdf_quartiers['centroid'] = gdf_quartiers.geometry.centroid\n",
    "gdf_quartiers = gdf_quartiers.to_crs(epsg=4326) # Retour au standard GPS\n",
    "\n",
    "# Extraction Lat/Lon des centroÃ¯des recalculÃ©s\n",
    "# Attention: AprÃ¨s reprojection, on accÃ¨de au centroid via la colonne qu'on a crÃ©Ã©e, mais il faut la reprojeter aussi\n",
    "# Plus simple : on recrÃ©e le centroid en 4326 directement si la prÃ©cision au mÃ¨tre n'est pas vitale, \n",
    "# mais pour Ãªtre propre, utilisons la colonne geometry reprojetÃ©e.\n",
    "gdf_quartiers['LATITUDE_ZONE'] = gdf_quartiers['centroid'].to_crs(epsg=4326).y\n",
    "gdf_quartiers['LONGITUDE_ZONE'] = gdf_quartiers['centroid'].to_crs(epsg=4326).x\n",
    "\n",
    "# Sauvegardes\n",
    "gdf_quartiers[['GEOCODE', 'GEONAME', 'BOROUGH', 'geometry']].to_file(\"dashboard_map.geojson\", driver='GeoJSON')\n",
    "pdf_locations = pd.DataFrame(gdf_quartiers[['GEOCODE', 'GEONAME', 'LATITUDE_ZONE', 'LONGITUDE_ZONE']])\n",
    "spark_locations = spark.createDataFrame(pdf_locations)\n",
    "\n",
    "# --- B. Traitement Air Quality (CORRIGÃ‰) ---\n",
    "print(\"   ðŸ’¨ Traitement Air Quality (Mode Manuel)...\")\n",
    "\n",
    "# Lecture manuelle car le JSON est une liste de listes (Socrata)\n",
    "with open(AIR_QUALITY_PATH, 'r') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# DÃ©finition du SchÃ©ma explicite (Index Socrata)\n",
    "# 17=Date, 14=GeoID, 10=Polluant, 18=Valeur\n",
    "fields = []\n",
    "for i in range(20): # On crÃ©e 20 colonnes gÃ©nÃ©riques\n",
    "    name = f\"col_{i}\"\n",
    "    if i == 17: name = \"DATE_MESURE_BRUTE\"\n",
    "    elif i == 14: name = \"GEOJOIN_ID_BRUT\"\n",
    "    elif i == 10: name = \"NOM_POLLUANT\"\n",
    "    elif i == 18: name = \"VALEUR_MESURE_BRUTE\"\n",
    "    fields.append(StructField(name, StringType(), True))\n",
    "\n",
    "schema = StructType(fields)\n",
    "\n",
    "# CrÃ©ation DataFrame Spark\n",
    "air_q_df = spark.createDataFrame(raw_data, schema=schema)\n",
    "\n",
    "# Nettoyage\n",
    "air_quality_clean = air_q_df.withColumn(\n",
    "    \"DATE_OBSERVATION\", \n",
    "    to_date(col(\"DATE_MESURE_BRUTE\"))\n",
    ").select(\n",
    "    col(\"GEOJOIN_ID_BRUT\").alias(\"GEOJOIN_ID\"),\n",
    "    col(\"DATE_OBSERVATION\"),\n",
    "    col(\"NOM_POLLUANT\"),\n",
    "    col(\"VALEUR_MESURE_BRUTE\").cast(DoubleType()).alias(\"VALEUR\")\n",
    ").filter(col(\"VALEUR\").isNotNull())\n",
    "\n",
    "# --- C. Jointure Air Quality + CoordonnÃ©es ---\n",
    "print(\"   ðŸ”— Jointure Air Quality + GÃ©ographie...\")\n",
    "final_air_data = air_quality_clean.join(\n",
    "    spark_locations,\n",
    "    air_quality_clean.GEOJOIN_ID == spark_locations.GEOCODE,\n",
    "    \"inner\"\n",
    ").drop(\"GEOCODE\")\n",
    "\n",
    "final_air_data.toPandas().to_parquet(\"dashboard_data_air.parquet\", index=False)\n",
    "\n",
    "# --- D. Traitement MÃ©tÃ©o ---\n",
    "print(\"   â˜€ï¸  Traitement MÃ©tÃ©o...\")\n",
    "weather_df = spark.read.parquet(WEATHER_PATH)\n",
    "weather_lite = weather_df.select(\"ID_STATION\", \"NAME\", \"LATITUDE\", \"LONGITUDE\", \"DATE\", \"TEMP\", \"DEWP\", \"WDSP\")\n",
    "weather_lite.toPandas().to_parquet(\"dashboard_data_weather.parquet\", index=False)\n",
    "\n",
    "print(\"âœ… ETL terminÃ© ! Fichiers gÃ©nÃ©rÃ©s.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Ã‰TAPE 2 : GÃ‰NÃ‰RATION DU DASHBOARD (Identique)\n",
    "# ==============================================================================\n",
    "dashboard_code = \"\"\"\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from streamlit_folium import st_folium\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "st.set_page_config(layout=\"wide\", page_title=\"NYC Environmental Dashboard\")\n",
    "\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    geo = gpd.read_file(\"dashboard_map.geojson\")\n",
    "    air = pd.read_parquet(\"dashboard_data_air.parquet\")\n",
    "    weather = pd.read_parquet(\"dashboard_data_weather.parquet\")\n",
    "    \n",
    "    air['DATE_OBSERVATION'] = pd.to_datetime(air['DATE_OBSERVATION'])\n",
    "    weather['DATE'] = pd.to_datetime(weather['DATE'])\n",
    "    \n",
    "    stations = weather[['ID_STATION', 'NAME', 'LATITUDE', 'LONGITUDE']].drop_duplicates()\n",
    "    return geo, air, weather, stations\n",
    "\n",
    "def haversine_dist(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    a = sin((lat2-lat1)/2)**2 + cos(lat1) * cos(lat2) * sin((lon2-lon1)/2)**2\n",
    "    return 6371 * 2 * asin(sqrt(a))\n",
    "\n",
    "def get_weighted_weather(target_lat, target_lon, weather_df, station_ids):\n",
    "    subset = weather_df[weather_df['ID_STATION'].isin(station_ids)].copy()\n",
    "    if subset.empty: return None\n",
    "    subset['dist'] = subset.apply(lambda x: haversine_dist(target_lon, target_lat, x['LONGITUDE'], x['LATITUDE']), axis=1)\n",
    "    subset['weight'] = 1 / (subset['dist'] + 0.1)\n",
    "    try:\n",
    "        return np.average(subset['TEMP'], weights=subset['weight'])\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "geo, df_air, df_weather, df_stations = load_data()\n",
    "\n",
    "st.sidebar.title(\"Filtres\")\n",
    "dates = st.sidebar.date_input(\"PÃ©riode\", [df_air['DATE_OBSERVATION'].min(), df_air['DATE_OBSERVATION'].max()])\n",
    "polluant = st.sidebar.selectbox(\"Polluant\", df_air['NOM_POLLUANT'].unique())\n",
    "\n",
    "mask_air = (df_air['DATE_OBSERVATION'].dt.date >= dates[0]) & (df_air['DATE_OBSERVATION'].dt.date <= dates[1])\n",
    "df_air_view = df_air[mask_air & (df_air['NOM_POLLUANT'] == polluant)]\n",
    "\n",
    "col_map, col_stats = st.columns([3, 2])\n",
    "\n",
    "with col_map:\n",
    "    st.subheader(\"Carte des Quartiers\")\n",
    "    m = folium.Map([40.7, -74.0], zoom_start=10)\n",
    "    folium.GeoJson(\n",
    "        geo, \n",
    "        name=\"Quartiers\",\n",
    "        tooltip=folium.GeoJsonTooltip(fields=['GEONAME', 'BOROUGH']),\n",
    "        style_function=lambda x: {'fillColor': '#3388ff', 'color': 'black', 'weight': 0.5, 'fillOpacity': 0.4},\n",
    "        highlight_function=lambda x: {'weight': 3, 'color': 'red'}\n",
    "    ).add_to(m)\n",
    "    st_map = st_folium(m, width=None, height=600)\n",
    "\n",
    "selected_geocode = None\n",
    "selected_name = st.sidebar.selectbox(\"Ou choisir un quartier :\", geo['GEONAME'].unique())\n",
    "q_data = geo[geo['GEONAME'] == selected_name].iloc[0]\n",
    "selected_geocode = q_data['GEOCODE']\n",
    "centroid = q_data.geometry.centroid\n",
    "lat_q, lon_q = centroid.y, centroid.x\n",
    "\n",
    "with col_stats:\n",
    "    st.title(selected_name)\n",
    "    st.info(f\"Borough: {q_data['BOROUGH']}\")\n",
    "    \n",
    "    st.markdown(\"### 1. QualitÃ© de l'Air\")\n",
    "    local_air = df_air_view[df_air_view['GEOJOIN_ID'] == selected_geocode]\n",
    "    if not local_air.empty:\n",
    "        st.metric(f\"Moyenne {polluant}\", f\"{local_air['VALEUR'].mean():.2f}\")\n",
    "        fig = px.bar(local_air, x='DATE_OBSERVATION', y='VALEUR', title=\"Ã‰volution\")\n",
    "        st.plotly_chart(fig, use_container_width=True)\n",
    "    else:\n",
    "        st.warning(\"Pas de donnÃ©es.\")\n",
    "\n",
    "    st.markdown(\"### 2. MÃ©tÃ©o PondÃ©rÃ©e\")\n",
    "    df_stations['dist'] = df_stations.apply(lambda x: haversine_dist(lon_q, lat_q, x['LONGITUDE'], x['LATITUDE']), axis=1)\n",
    "    df_stations = df_stations.sort_values('dist')\n",
    "    sel_stations = st.multiselect(\"Stations\", df_stations['ID_STATION'].tolist(), format_func=lambda x: f\"{df_stations[df_stations.ID_STATION==x].iloc[0]['NAME']} ({df_stations[df_stations.ID_STATION==x].iloc[0]['dist']:.1f}km)\")\n",
    "    \n",
    "    if sel_stations:\n",
    "        mask_w = (df_weather['DATE'].dt.date >= dates[0]) & (df_weather['DATE'].dt.date <= dates[1])\n",
    "        w_temp = get_weighted_weather(lat_q, lon_q, df_weather[mask_w], sel_stations)\n",
    "        if w_temp: st.metric(\"TempÃ©rature PondÃ©rÃ©e\", f\"{w_temp:.1f} Â°F\")\n",
    "\"\"\"\n",
    "\n",
    "with open(\"app.py\", \"w\", encoding='utf-8') as f:\n",
    "    f.write(dashboard_code)\n",
    "\n",
    "print(\"\\nðŸš€ Application gÃ©nÃ©rÃ©e ! Lancez dans le terminal : streamlit run app.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
