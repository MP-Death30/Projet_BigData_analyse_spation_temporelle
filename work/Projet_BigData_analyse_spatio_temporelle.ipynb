{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b0bef17-e0d3-4a2e-a48a-f87ab105bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biblioth√®ques standard\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "# Biblioth√®ques tierces\n",
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from IPython.display import display\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import to_date, col, lit, substring, regexp_replace\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac6f5db-9076-4f36-9499-56c0fc9306c7",
   "metadata": {},
   "source": [
    "# Pr√©paration des donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aca566-92f4-464c-9c85-95fd802477d1",
   "metadata": {},
   "source": [
    "## Phase 1 : Initialisation de l'Environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "408cc07a-fda5-4d7b-9ef3-b6b641a1014c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Session Spark configur√©e et initialis√©e.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Configuration et Initialisation de Spark ---\n",
    "# Augmentation des timeouts et allocation de m√©moire stricte pour √©viter les crashs JVM\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataLake_NOAA_NYC_Prep\") \\\n",
    "    .config(\"spark.executor.memory\", \"3g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.rpc.askTimeout\", \"800s\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# --- 2. D√©finition des Param√®tres G√©ographiques et HDFS ---\n",
    "# Bo√Æte englobante de la r√©gion de NYC afin de restreindre l'import des donn√©es NOAA\n",
    "MIN_LAT, MAX_LAT = 40.0, 41.5\n",
    "MIN_LON, MAX_LON = -75.0, -73.0\n",
    "\n",
    "# Chemin HDFS BRUT\n",
    "RAW_OUTPUT_PATH = \"hdfs://namenode:9000/user/mathis/datalake/noaa_gsod_nyc_raw_2005_2023.parquet\"\n",
    "\n",
    "# Plage d'ann√©es\n",
    "START_YEAR = 2005\n",
    "END_YEAR = 2023\n",
    "\n",
    "print(\"‚úÖ Session Spark configur√©e et initialis√©e.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b968a45-1bcf-4648-9174-5232e8494a1a",
   "metadata": {},
   "source": [
    "## Phase 2 : M√©tadonn√©es et Identification des Stations NOAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5465fda2-fbcf-433b-b657-abf861fbdb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ 93 Stations NOAA pertinentes trouv√©es pr√®s de New York.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. T√©l√©chargement des M√©tadonn√©es des Stations ---\n",
    "stations_url = \"https://www.ncei.noaa.gov/pub/data/noaa/isd-history.csv\"\n",
    "pdf_stations = pd.read_csv(stations_url,\n",
    "                         dtype={'USAF': str, 'WBAN': str})\n",
    "\n",
    "pdf_stations['STN_ID'] = pdf_stations['USAF'].str.strip() + pdf_stations['WBAN'].str.strip()\n",
    "pdf_stations = pdf_stations.rename(columns={'LAT': 'LATITUDE', 'LON': 'LONGITUDE'})\n",
    "pdf_stations = pdf_stations.dropna(subset=['LATITUDE', 'LONGITUDE', 'STATION NAME'])\n",
    "spark_stations_df = spark.createDataFrame(pdf_stations)\n",
    "\n",
    "# --- 2. Filtrage G√©ographique ---\n",
    "nyc_stations_spark = spark_stations_df.filter(\n",
    "    (F.col('LATITUDE') >= MIN_LAT) & (F.col('LATITUDE') <= MAX_LAT) &\n",
    "    (F.col('LONGITUDE') >= MIN_LON) & (F.col('LONGITUDE') <= MAX_LON)\n",
    ")\n",
    "\n",
    "# R√©cup√©ration de la liste des IDs pertinents (pour filtrage par nom de fichier)\n",
    "relevant_station_ids = [row.STN_ID for row in nyc_stations_spark.select(\"STN_ID\").collect()]\n",
    "\n",
    "print(f\"\\n‚úÖ {nyc_stations_spark.count()} Stations NOAA pertinentes trouv√©es pr√®s de New York.\")\n",
    "# Gardons ce DataFrame pour la jointure des coordonn√©es plus tard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68963a2-8016-4469-9211-9cfbf0794c01",
   "metadata": {},
   "source": [
    "## Phase 2 bis : T√©l√©chargement des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbad7f3a-e433-4507-8618-e729106a53a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D√©marrage du t√©l√©chargement pour 93 stations de 2005 √† 2023.\n",
      "Traitement de l'ann√©e : 2005...\n",
      "Traitement de l'ann√©e : 2006...\n",
      "Traitement de l'ann√©e : 2007...\n",
      "Traitement de l'ann√©e : 2008...\n",
      "Traitement de l'ann√©e : 2009...\n",
      "Traitement de l'ann√©e : 2010...\n",
      "Traitement de l'ann√©e : 2011...\n",
      "Traitement de l'ann√©e : 2012...\n",
      "Traitement de l'ann√©e : 2013...\n",
      "Traitement de l'ann√©e : 2014...\n",
      "Traitement de l'ann√©e : 2015...\n",
      "Traitement de l'ann√©e : 2016...\n",
      "Traitement de l'ann√©e : 2017...\n",
      "Traitement de l'ann√©e : 2018...\n",
      "Traitement de l'ann√©e : 2019...\n",
      "Traitement de l'ann√©e : 2020...\n",
      "Traitement de l'ann√©e : 2021...\n",
      "Traitement de l'ann√©e : 2022...\n",
      "Traitement de l'ann√©e : 2023...\n",
      "\n",
      "‚úÖ T√©l√©chargement termin√©. 552 fichiers GSOD trait√©s (t√©l√©charg√©s ou existants).\n"
     ]
    }
   ],
   "source": [
    "BASE_URL = \"https://www.ncei.noaa.gov/data/global-summary-of-the-day/access\"\n",
    "LOCAL_BASE_DIR = \"/home/jovyan/work/data/noaa_gsod\"\n",
    "START_YEAR = 2005\n",
    "END_YEAR = 2023\n",
    "\n",
    "# --- IDs des stations √† t√©l√©charger ---\n",
    "if 'relevant_station_ids' not in locals():\n",
    "    print(\"‚ö†Ô∏è ATTENTION: La liste 'relevant_station_ids' n'est pas d√©finie. Veuillez ex√©cuter la Phase 2 en premier.\")\n",
    "    exit()\n",
    "\n",
    "# D√©marrage du processus\n",
    "print(f\"D√©marrage du t√©l√©chargement pour {len(relevant_station_ids)} stations de {START_YEAR} √† {END_YEAR}.\")\n",
    "\n",
    "downloaded_count = 0\n",
    "\n",
    "# --- Boucle principale (sans barre de progression) ---\n",
    "for year in range(START_YEAR, END_YEAR + 1):\n",
    "    year_dir = os.path.join(LOCAL_BASE_DIR, str(year))\n",
    "    \n",
    "    # Petit print pour savoir o√π on en est (optionnel, mais utile sans barre de progression)\n",
    "    print(f\"Traitement de l'ann√©e : {year}...\")\n",
    "\n",
    "    # Cr√©e le r√©pertoire de l'ann√©e s'il n'existe pas\n",
    "    os.makedirs(year_dir, exist_ok=True)\n",
    "\n",
    "    for station_id in relevant_station_ids:\n",
    "        file_name = f\"{station_id}.csv\"\n",
    "        local_path = os.path.join(year_dir, file_name)\n",
    "        remote_url = f\"{BASE_URL}/{year}/{file_name}\"\n",
    "\n",
    "        # V√©rifie si le fichier existe d√©j√†\n",
    "        if os.path.exists(local_path):\n",
    "            downloaded_count += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Requ√™te HTTP GET\n",
    "            response = requests.get(remote_url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # √âcrit le contenu dans le fichier local\n",
    "            with open(local_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "            downloaded_count += 1\n",
    "            \n",
    "            # Pause pour √™tre poli avec le serveur NOAA\n",
    "            time.sleep(0.05) \n",
    "\n",
    "        except requests.exceptions.HTTPError as errh:\n",
    "            # Fichier 404/Not Found\n",
    "            if response.status_code == 404:\n",
    "                pass \n",
    "            else:\n",
    "                print(f\"\\n‚ùå Erreur HTTP pour {remote_url}: {errh}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"\\n‚ùå Erreur de Connexion/Timeout pour {remote_url}: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ T√©l√©chargement termin√©. {downloaded_count} fichiers GSOD trait√©s (t√©l√©charg√©s ou existants).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ba3823-17a9-4fba-bc32-51894644af18",
   "metadata": {},
   "source": [
    "## Phase 3 : Ingestion Cibl√©e et Persistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c1aaaca-7e8e-4e8f-93ce-32027b3f8c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de 552 fichiers existants seront lus par Spark.\n",
      "\n",
      "Sauvegarde de la copie BRUTE filtr√©e (2005-2023) dans : hdfs://namenode:9000/user/mathis/datalake/noaa_gsod_nyc_raw_2005_2023.parquet...\n",
      "‚úÖ Copie brute sauvegard√©e sur HDFS. Le traitement peut se poursuivre.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration des Chemins ---\n",
    "LOCAL_BASE_DIR = \"/home/jovyan/work/data/noaa_gsod\" \n",
    "RAW_OUTPUT_PATH = \"hdfs://namenode:9000/user/mathis/datalake/noaa_gsod_nyc_raw_2005_2023.parquet\"\n",
    "START_YEAR = 2005\n",
    "END_YEAR = 2023\n",
    "\n",
    "# --- 1. D√©finition des Chemins Cibl√©s ---\n",
    "# Nous recr√©ons la liste, mais cette fois en utilisant 'glob' ou une v√©rification OS\n",
    "# pour ne pas inclure les chemins qui n'existent pas.\n",
    "\n",
    "existing_targeted_paths = []\n",
    "for year in range(START_YEAR, END_YEAR + 1):\n",
    "    for station_id in relevant_station_ids:\n",
    "        # Chemin absolu corrig√© : /home/jovyan/work/data/noaa_gsod/2005/XXXXX.csv\n",
    "        path = f\"{LOCAL_BASE_DIR}/{year}/{station_id}.csv\"\n",
    "        \n",
    "        # V√©rifie si le fichier existe vraiment avant de l'ajouter √† la liste de lecture de Spark\n",
    "        if os.path.exists(path):\n",
    "            existing_targeted_paths.append(path)\n",
    "\n",
    "# Si aucun chemin n'existe, nous aurons une erreur, mais au moins nous savons pourquoi.\n",
    "if not existing_targeted_paths:\n",
    "    raise FileNotFoundError(\"Aucun fichier GSOD cible n'a √©t√© trouv√© dans le r√©pertoire local.\")\n",
    "\n",
    "gsod_data_paths = existing_targeted_paths\n",
    "print(f\"Total de {len(gsod_data_paths)} fichiers existants seront lus par Spark.\")\n",
    "\n",
    "# --- 2. Sch√©ma et Lecture ---\n",
    "gsod_schema = StructType([\n",
    "    StructField(\"STATION\", StringType(), True),\n",
    "    StructField(\"DATE\", StringType(), True),\n",
    "    StructField(\"LATITUDE\", DoubleType(), True), \n",
    "    StructField(\"LONGITUDE\", DoubleType(), True),\n",
    "    StructField(\"ELEVATION\", DoubleType(), True),\n",
    "    StructField(\"NAME\", StringType(), True),\n",
    "    StructField(\"TEMP\", DoubleType(), True),\n",
    "    StructField(\"TEMP_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"DEWP\", DoubleType(), True),\n",
    "    StructField(\"DEWP_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"SLP\", DoubleType(), True),\n",
    "    StructField(\"SLP_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"STP\", DoubleType(), True),\n",
    "    StructField(\"STP_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"VISIB\", DoubleType(), True),\n",
    "    StructField(\"VISIB_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"WDSP\", DoubleType(), True),\n",
    "    StructField(\"WDSP_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"MXSPD\", DoubleType(), True),\n",
    "    StructField(\"GUST\", DoubleType(), True),\n",
    "    StructField(\"MAX\", DoubleType(), True),\n",
    "    StructField(\"MAX_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"MIN\", DoubleType(), True),\n",
    "    StructField(\"MIN_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"PRCP\", DoubleType(), True),\n",
    "    StructField(\"PRCP_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"SNDP\", DoubleType(), True),\n",
    "    StructField(\"FRSHHT\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Lecture distribu√©e des donn√©es GSOD (seulement les fichiers cibl√©s)\n",
    "all_gsod_data = spark.read.csv(\n",
    "    gsod_data_paths,\n",
    "    header=True,\n",
    "    schema=gsod_schema,\n",
    "    sep=','\n",
    ")\n",
    "\n",
    "# Renommage de la colonne ID\n",
    "nyc_gsod_data = all_gsod_data.withColumnRenamed(\"STATION\", \"ID_STATION\")\n",
    "\n",
    "\n",
    "# --- 3. Persistance de la Couche Brute sur HDFS ---\n",
    "print(f\"\\nSauvegarde de la copie BRUTE filtr√©e (2005-2023) dans : {RAW_OUTPUT_PATH}...\")\n",
    "# Cette √©tape transf√®re les donn√©es du disque local du conteneur vers HDFS\n",
    "nyc_gsod_data.write.mode(\"overwrite\").parquet(RAW_OUTPUT_PATH)\n",
    "print(\"‚úÖ Copie brute sauvegard√©e sur HDFS. Le traitement peut se poursuivre.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf6aee1-d259-469a-ac59-2bbf8b3e3ed1",
   "metadata": {},
   "source": [
    "## Phase 4 : T√©l√©chargement et Nettoyage du JSON Socrata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c8fa00f-a310-42a8-935e-4da50409c6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚¨áÔ∏è T√©l√©chargement du JSON Socrata depuis l'API de NYC...\n",
      "‚úÖ Fichier JSON brut sauvegard√© et nettoy√© structurellement √† : /home/jovyan/work/data/air_quality/nyc_air_quality_raw.json\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration des Chemins ---\n",
    "LOCAL_BASE_DIR = \"/home/jovyan/work/data/air_quality\"\n",
    "LOCAL_JSON_PATH = os.path.join(LOCAL_BASE_DIR, \"nyc_air_quality_raw.json\")\n",
    "AIR_QUALITY_URL = \"https://data.cityofnewyork.us/api/views/c3uy-2p5r/rows.json?accessType=DOWNLOAD\"\n",
    "\n",
    "# Cr√©e le r√©pertoire local si n√©cessaire\n",
    "os.makedirs(LOCAL_BASE_DIR, exist_ok=True)\n",
    "\n",
    "# --- 1. T√©l√©chargement et Nettoyage de la structure JSON Socrata ---\n",
    "print(f\"‚¨áÔ∏è T√©l√©chargement du JSON Socrata depuis l'API de NYC...\")\n",
    "try:\n",
    "    response = requests.get(AIR_QUALITY_URL, timeout=300) # Timeout de 5 minutes\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    \n",
    "    # La cl√© 'data' contient le tableau des enregistrements bruts que Spark doit lire.\n",
    "    raw_records = data.get('data', [])\n",
    "\n",
    "    if not raw_records:\n",
    "        print(\"‚ùå Erreur : La cl√© 'data' est vide dans le JSON t√©l√©charg√©. Arr√™t du processus.\")\n",
    "        exit()\n",
    "    \n",
    "    # √âcriture du tableau de donn√©es brutes SEULEMENT dans le nouveau fichier JSON.\n",
    "    # Ceci est essentiel pour que le RDD/toDF fonctionne correctement.\n",
    "    with open(LOCAL_JSON_PATH, 'w') as f:\n",
    "        json.dump(raw_records, f)\n",
    "\n",
    "    print(f\"‚úÖ Fichier JSON brut sauvegard√© et nettoy√© structurellement √† : {LOCAL_JSON_PATH}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors du t√©l√©chargement/nettoyage : {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbcdc9e-f279-4229-bfd7-79f1e1ea8fa9",
   "metadata": {},
   "source": [
    "## Phase 5 - Transformation des donn√©es et Cr√©ation du Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b611b3e-eeeb-4eb5-80c6-b3e04f2e3c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ D√©marrage de l'ETL Spark...\n",
      "   üìç Traitement du GeoJSON...\n",
      "   üí® Traitement Air Quality (Mode Manuel)...\n",
      "   üîó Jointure Air Quality + G√©ographie...\n",
      "   ‚òÄÔ∏è  Traitement M√©t√©o...\n",
      "‚úÖ ETL termin√© ! Fichiers g√©n√©r√©s.\n",
      "\n",
      "üöÄ Application g√©n√©r√©e ! Lancez dans le terminal : streamlit run app.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, col, lit, substring, regexp_replace\n",
    "from pyspark.sql.types import DoubleType, StructType, StructField, StringType\n",
    "\n",
    "# ==============================================================================\n",
    "# √âTAPE 1 : PR√âPARATION DES DONN√âES (ETL SPARK)\n",
    "# ==============================================================================\n",
    "print(\"üîÑ D√©marrage de l'ETL Spark...\")\n",
    "\n",
    "# 1. Initialisation Spark\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    spark = SparkSession.builder.appName(\"OneCode_ETL\").getOrCreate()\n",
    "\n",
    "# 2. Configuration des chemins\n",
    "GEOJSON_URL = \"https://raw.githubusercontent.com/nycehs/NYC_geography/master/UHF42.geo.json\"\n",
    "AIR_QUALITY_PATH = \"/home/jovyan/work/data/air_quality/nyc_air_quality_raw.json\" \n",
    "WEATHER_PATH = \"hdfs://namenode:9000/user/mathis/datalake/noaa_gsod_nyc_raw_2005_2023.parquet\"\n",
    "\n",
    "# --- A. Traitement G√©ographique (GeoJSON) ---\n",
    "print(\"   üìç Traitement du GeoJSON...\")\n",
    "gdf_quartiers = gpd.read_file(GEOJSON_URL)\n",
    "\n",
    "# Correction G√©om√©trique : On projette en EPSG:2263 (NYC Feet) pour calculer le centre, puis on revient en Lat/Lon\n",
    "# Cela √©vite le warning et donne un centre plus pr√©cis.\n",
    "gdf_quartiers = gdf_quartiers.to_crs(epsg=2263) \n",
    "gdf_quartiers['centroid'] = gdf_quartiers.geometry.centroid\n",
    "gdf_quartiers = gdf_quartiers.to_crs(epsg=4326) # Retour au standard GPS\n",
    "\n",
    "# Extraction Lat/Lon des centro√Ødes recalcul√©s\n",
    "# Attention: Apr√®s reprojection, on acc√®de au centroid via la colonne qu'on a cr√©√©e, mais il faut la reprojeter aussi\n",
    "# Plus simple : on recr√©e le centroid en 4326 directement si la pr√©cision au m√®tre n'est pas vitale, \n",
    "# mais pour √™tre propre, utilisons la colonne geometry reprojet√©e.\n",
    "gdf_quartiers['LATITUDE_ZONE'] = gdf_quartiers['centroid'].to_crs(epsg=4326).y\n",
    "gdf_quartiers['LONGITUDE_ZONE'] = gdf_quartiers['centroid'].to_crs(epsg=4326).x\n",
    "\n",
    "# Sauvegardes\n",
    "gdf_quartiers[['GEOCODE', 'GEONAME', 'BOROUGH', 'geometry']].to_file(\"dashboard_map.geojson\", driver='GeoJSON')\n",
    "pdf_locations = pd.DataFrame(gdf_quartiers[['GEOCODE', 'GEONAME', 'LATITUDE_ZONE', 'LONGITUDE_ZONE']])\n",
    "spark_locations = spark.createDataFrame(pdf_locations)\n",
    "\n",
    "# --- B. Traitement Air Quality (CORRIG√â) ---\n",
    "print(\"   üí® Traitement Air Quality (Mode Manuel)...\")\n",
    "\n",
    "# Lecture manuelle car le JSON est une liste de listes (Socrata)\n",
    "with open(AIR_QUALITY_PATH, 'r') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# D√©finition du Sch√©ma explicite (Index Socrata)\n",
    "# 17=Date, 14=GeoID, 10=Polluant, 18=Valeur\n",
    "fields = []\n",
    "for i in range(20): # On cr√©e 20 colonnes g√©n√©riques\n",
    "    name = f\"col_{i}\"\n",
    "    if i == 17: name = \"DATE_MESURE_BRUTE\"\n",
    "    elif i == 14: name = \"GEOJOIN_ID_BRUT\"\n",
    "    elif i == 10: name = \"NOM_POLLUANT\"\n",
    "    elif i == 18: name = \"VALEUR_MESURE_BRUTE\"\n",
    "    fields.append(StructField(name, StringType(), True))\n",
    "\n",
    "schema = StructType(fields)\n",
    "\n",
    "# Cr√©ation DataFrame Spark\n",
    "air_q_df = spark.createDataFrame(raw_data, schema=schema)\n",
    "\n",
    "# Nettoyage\n",
    "air_quality_clean = air_q_df.withColumn(\n",
    "    \"DATE_OBSERVATION\", \n",
    "    to_date(col(\"DATE_MESURE_BRUTE\"))\n",
    ").select(\n",
    "    col(\"GEOJOIN_ID_BRUT\").alias(\"GEOJOIN_ID\"),\n",
    "    col(\"DATE_OBSERVATION\"),\n",
    "    col(\"NOM_POLLUANT\"),\n",
    "    col(\"VALEUR_MESURE_BRUTE\").cast(DoubleType()).alias(\"VALEUR\")\n",
    ").filter(col(\"VALEUR\").isNotNull())\n",
    "\n",
    "# --- C. Jointure Air Quality + Coordonn√©es ---\n",
    "print(\"   üîó Jointure Air Quality + G√©ographie...\")\n",
    "final_air_data = air_quality_clean.join(\n",
    "    spark_locations,\n",
    "    air_quality_clean.GEOJOIN_ID == spark_locations.GEOCODE,\n",
    "    \"inner\"\n",
    ").drop(\"GEOCODE\")\n",
    "\n",
    "final_air_data.toPandas().to_parquet(\"dashboard_data_air.parquet\", index=False)\n",
    "\n",
    "# --- D. Traitement M√©t√©o ---\n",
    "print(\"   ‚òÄÔ∏è  Traitement M√©t√©o...\")\n",
    "weather_df = spark.read.parquet(WEATHER_PATH)\n",
    "weather_lite = weather_df.select(\"ID_STATION\", \"NAME\", \"LATITUDE\", \"LONGITUDE\", \"DATE\", \"TEMP\", \"DEWP\", \"WDSP\")\n",
    "weather_lite.toPandas().to_parquet(\"dashboard_data_weather.parquet\", index=False)\n",
    "\n",
    "print(\"‚úÖ ETL termin√© ! Fichiers g√©n√©r√©s.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# √âTAPE 2 : G√âN√âRATION DU DASHBOARD (Identique)\n",
    "# ==============================================================================\n",
    "dashboard_code = \"\"\"\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from streamlit_folium import st_folium\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURATION & INITIALISATION\n",
    "# ==============================================================================\n",
    "st.set_page_config(layout=\"wide\", page_title=\"NYC Environmental Dashboard\")\n",
    "\n",
    "# Initialisation des √©tats\n",
    "if 'selected_geocode' not in st.session_state:\n",
    "    st.session_state.selected_geocode = None\n",
    "if 'dropdown_selector' not in st.session_state:\n",
    "    st.session_state.dropdown_selector = \"Tous quartiers\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. FONCTIONS UTILITAIRES\n",
    "# ==============================================================================\n",
    "\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    geo = gpd.read_file(\"dashboard_map.geojson\")\n",
    "    air = pd.read_parquet(\"dashboard_data_air.parquet\")\n",
    "    weather = pd.read_parquet(\"dashboard_data_weather.parquet\")\n",
    "    \n",
    "    # --- NETTOYAGE & CONVERSIONS ---\n",
    "    geo['GEOCODE'] = geo['GEOCODE'].astype(str)\n",
    "    \n",
    "    if 'LATITUDE_ZONE' not in geo.columns:\n",
    "        try:\n",
    "            geo_temp = geo.to_crs(epsg=2263)\n",
    "            centroids = geo_temp.geometry.centroid.to_crs(epsg=4326)\n",
    "        except:\n",
    "            centroids = geo.geometry.centroid\n",
    "        geo['LATITUDE_ZONE'] = centroids.y\n",
    "        geo['LONGITUDE_ZONE'] = centroids.x\n",
    "    \n",
    "    air['DATE_OBSERVATION'] = pd.to_datetime(air['DATE_OBSERVATION'])\n",
    "    weather['DATE'] = pd.to_datetime(weather['DATE'])\n",
    "    \n",
    "    # Conversion Unit√©s\n",
    "    weather['TEMP'] = (weather['TEMP'] - 32) * 5.0/9.0\n",
    "    weather['DEWP'] = (weather['DEWP'] - 32) * 5.0/9.0\n",
    "    weather['WDSP'] = weather['WDSP'] * 1.852\n",
    "    \n",
    "    stations = weather[['ID_STATION', 'NAME', 'LATITUDE', 'LONGITUDE']].drop_duplicates()\n",
    "    return geo, air, weather, stations\n",
    "\n",
    "def haversine_vectorized(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return 6371 * c \n",
    "\n",
    "def calculate_global_metrics(geo_df, weather_df, stations_df, radius_km):\n",
    "    results = []\n",
    "    weather_agg = weather_df.groupby('ID_STATION')[['TEMP', 'WDSP', 'DEWP']].mean().reset_index()\n",
    "    stations_w_weather = stations_df.merge(weather_agg, on='ID_STATION')\n",
    "    \n",
    "    if stations_w_weather.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    for _, row in geo_df.iterrows():\n",
    "        lat_q, lon_q = row['LATITUDE_ZONE'], row['LONGITUDE_ZONE']\n",
    "        dists = haversine_vectorized(\n",
    "            lon_q, lat_q, \n",
    "            stations_w_weather['LONGITUDE'].values, \n",
    "            stations_w_weather['LATITUDE'].values\n",
    "        )\n",
    "        \n",
    "        mask = dists <= radius_km\n",
    "        nearby_stations = stations_w_weather[mask].copy()\n",
    "        nearby_dists = dists[mask]\n",
    "        \n",
    "        if not nearby_stations.empty:\n",
    "            weights = 1 / (nearby_dists + 0.1)\n",
    "            w_temp = np.average(nearby_stations['TEMP'], weights=weights)\n",
    "            w_wind = np.average(nearby_stations['WDSP'], weights=weights)\n",
    "            w_dewp = np.average(nearby_stations['DEWP'], weights=weights)\n",
    "            \n",
    "            results.append({\n",
    "                'GEOCODE': str(row['GEOCODE']),\n",
    "                'W_TEMP': round(w_temp, 1),\n",
    "                'W_WIND': round(w_wind, 1),\n",
    "                'W_DEWP': round(w_dewp, 1),\n",
    "                'NB_STATIONS': len(nearby_stations)\n",
    "            })\n",
    "        else:\n",
    "            results.append({\n",
    "                'GEOCODE': str(row['GEOCODE']),\n",
    "                'W_TEMP': None, 'W_WIND': None, 'W_DEWP': None, 'NB_STATIONS': 0\n",
    "            })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. CHARGEMENT & FILTRES\n",
    "# ==============================================================================\n",
    "\n",
    "geo, df_air, df_weather, df_stations = load_data()\n",
    "\n",
    "st.sidebar.header(\"üéõÔ∏è Filtres & Param√®tres\")\n",
    "\n",
    "# Dates\n",
    "min_date, max_date = df_air['DATE_OBSERVATION'].min(), df_air['DATE_OBSERVATION'].max()\n",
    "start_date, end_date = st.sidebar.date_input(\n",
    "    \"P√©riode d'analyse\", [min_date, max_date], min_value=min_date, max_value=max_date\n",
    ")\n",
    "\n",
    "# Filtre Polluants Disponibles\n",
    "mask_air_date = (df_air['DATE_OBSERVATION'].dt.date >= start_date) & (df_air['DATE_OBSERVATION'].dt.date <= end_date)\n",
    "df_air_filtered = df_air[mask_air_date]\n",
    "\n",
    "valid_pollutants = df_air_filtered[df_air_filtered['VALEUR'].notna()]['NOM_POLLUANT'].unique()\n",
    "valid_pollutants = sorted(valid_pollutants)\n",
    "\n",
    "if len(valid_pollutants) > 0:\n",
    "    selected_polluant = st.sidebar.selectbox(\"Polluant (Dispo sur la p√©riode)\", valid_pollutants)\n",
    "else:\n",
    "    st.sidebar.error(\"‚ö†Ô∏è Aucune donn√©e de pollution pour cette p√©riode.\")\n",
    "    selected_polluant = None\n",
    "\n",
    "# Autres Filtres\n",
    "radius = st.sidebar.slider(\"Rayon des stations m√©t√©o (km)\", 1, 100, 15)\n",
    "meteo_vars = ['Temp√©rature', 'Vitesse Vent', 'Point de Ros√©e']\n",
    "selected_meteo_vars = st.sidebar.multiselect(\"Graphiques M√©t√©o (Comparaison)\", meteo_vars, default=['Temp√©rature'])\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. ETL A LA VOL√âE (PR√âPARATION GLOBALE)\n",
    "# ==============================================================================\n",
    "\n",
    "if selected_polluant is None:\n",
    "    st.warning(\"Veuillez √©largir la plage de dates.\")\n",
    "    st.stop()\n",
    "\n",
    "# Filtre M√©t√©o\n",
    "mask_weather_date = (df_weather['DATE'].dt.date >= start_date) & (df_weather['DATE'].dt.date <= end_date)\n",
    "df_weather_filtered = df_weather[mask_weather_date]\n",
    "\n",
    "# --- INDICATEUR SIDEBAR ---\n",
    "with st.sidebar:\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"### ‚ÑπÔ∏è Info Stations\")\n",
    "    active_stations = df_weather_filtered[['ID_STATION']].drop_duplicates()\n",
    "    active_stations_coords = active_stations.merge(df_stations, on='ID_STATION')\n",
    "\n",
    "    if st.session_state.selected_geocode is None:\n",
    "        # Mode Global : Centre NYC\n",
    "        lat_center = geo['LATITUDE_ZONE'].mean()\n",
    "        lon_center = geo['LONGITUDE_ZONE'].mean()\n",
    "        if not active_stations_coords.empty:\n",
    "            dists_s = haversine_vectorized(lon_center, lat_center, active_stations_coords['LONGITUDE'].values, active_stations_coords['LATITUDE'].values)\n",
    "            nb_visible = np.sum(dists_s <= radius)\n",
    "            st.metric(f\"Stations (Centre NYC, {radius} km)\", nb_visible)\n",
    "        else:\n",
    "            st.metric(f\"Stations (Centre NYC, {radius} km)\", 0)\n",
    "    else:\n",
    "        # Mode Local\n",
    "        sel_geo = geo[geo['GEOCODE'] == st.session_state.selected_geocode]\n",
    "        if not sel_geo.empty:\n",
    "            lat_s = sel_geo.iloc[0]['LATITUDE_ZONE']\n",
    "            lon_s = sel_geo.iloc[0]['LONGITUDE_ZONE']\n",
    "            if not active_stations_coords.empty:\n",
    "                dists_s = haversine_vectorized(lon_s, lat_s, active_stations_coords['LONGITUDE'].values, active_stations_coords['LATITUDE'].values)\n",
    "                nb_visible = np.sum(dists_s <= radius)\n",
    "                st.metric(f\"Stations (Quartier, {radius} km)\", nb_visible)\n",
    "            else:\n",
    "                st.metric(f\"Stations (Quartier, {radius} km)\", 0)\n",
    "\n",
    "# --- PR√âPARATION DONN√âES CARTE ---\n",
    "df_air_map = df_air_filtered[df_air_filtered['NOM_POLLUANT'] == selected_polluant]\n",
    "if not df_air_map.empty:\n",
    "    air_agg = df_air_map.groupby('GEOJOIN_ID')['VALEUR'].mean().reset_index()\n",
    "    air_agg.columns = ['GEOCODE', 'MEAN_POLLUANT']\n",
    "    air_agg['GEOCODE'] = air_agg['GEOCODE'].astype(str)\n",
    "else:\n",
    "    air_agg = pd.DataFrame(columns=['GEOCODE', 'MEAN_POLLUANT'])\n",
    "\n",
    "weather_metrics_df = calculate_global_metrics(geo, df_weather_filtered, df_stations, radius)\n",
    "\n",
    "gdf_display = geo.merge(air_agg, on='GEOCODE', how='left')\n",
    "if not weather_metrics_df.empty:\n",
    "    gdf_display = gdf_display.merge(weather_metrics_df, on='GEOCODE', how='left')\n",
    "\n",
    "gdf_display['MEAN_POLLUANT'] = gdf_display['MEAN_POLLUANT'].fillna(0).round(2)\n",
    "gdf_display['W_TEMP'] = gdf_display['W_TEMP'].fillna(0)\n",
    "gdf_display['NB_STATIONS'] = gdf_display['NB_STATIONS'].fillna(0).astype(int)\n",
    "\n",
    "# --- PR√âPARATION DONN√âES GRAPHIQUES & KPIs (AVANT AFFICHAGE) ---\n",
    "# Cela permet d'avoir les variables pr√™tes pour les deux colonnes\n",
    "\n",
    "current_title = \"\"\n",
    "current_caption = \"\"\n",
    "avg_polluant, avg_temp, avg_wind = 0, 0, 0\n",
    "\n",
    "# Sources de donn√©es brutes pour les graphes (√† filtrer selon s√©lection)\n",
    "chart_air_src = pd.DataFrame()\n",
    "chart_weather_src = pd.DataFrame()\n",
    "\n",
    "if st.session_state.selected_geocode is None:\n",
    "    # GLOBAL\n",
    "    current_title = \"New York City (Global)\"\n",
    "    current_caption = \"Moyenne de tous les quartiers\"\n",
    "    \n",
    "    valid_data = gdf_display[gdf_display['MEAN_POLLUANT'] > 0]\n",
    "    if not valid_data.empty:\n",
    "        avg_polluant = valid_data['MEAN_POLLUANT'].mean()\n",
    "        avg_temp = valid_data['W_TEMP'].replace(0, np.nan).mean()\n",
    "        avg_wind = valid_data['W_WIND'].replace(0, np.nan).mean()\n",
    "    \n",
    "    chart_air_src = df_air_filtered[df_air_filtered['NOM_POLLUANT'] == selected_polluant].copy()\n",
    "    chart_weather_src = df_weather_filtered.copy()\n",
    "else:\n",
    "    # LOCAL\n",
    "    current_geo_data = gdf_display[gdf_display['GEOCODE'] == st.session_state.selected_geocode].iloc[0]\n",
    "    current_title = current_geo_data['GEONAME']\n",
    "    current_caption = f\"Borough: {current_geo_data['BOROUGH']} | Stations locales : {int(current_geo_data['NB_STATIONS'])}\"\n",
    "    \n",
    "    avg_polluant = current_geo_data['MEAN_POLLUANT']\n",
    "    avg_temp = current_geo_data['W_TEMP']\n",
    "    avg_wind = current_geo_data['W_WIND']\n",
    "    \n",
    "    chart_air_src = df_air_filtered[\n",
    "        (df_air_filtered['GEOJOIN_ID'] == st.session_state.selected_geocode) & \n",
    "        (df_air_filtered['NOM_POLLUANT'] == selected_polluant)\n",
    "    ].copy()\n",
    "    \n",
    "    lat_q, lon_q = current_geo_data['LATITUDE_ZONE'], current_geo_data['LONGITUDE_ZONE']\n",
    "    dists = haversine_vectorized(lon_q, lat_q, df_stations['LONGITUDE'].values, df_stations['LATITUDE'].values)\n",
    "    nearby_ids = df_stations[dists <= radius]['ID_STATION'].unique()\n",
    "    chart_weather_src = df_weather_filtered[df_weather_filtered['ID_STATION'].isin(nearby_ids)].copy()\n",
    "\n",
    "# RESAMPLING COMMUN\n",
    "delta_days = (end_date - start_date).days\n",
    "resample_rule = 'D'\n",
    "if delta_days > 730: resample_rule = 'Q'\n",
    "elif delta_days > 180: resample_rule = 'M'\n",
    "elif delta_days > 60: resample_rule = 'W'\n",
    "\n",
    "if not chart_air_src.empty:\n",
    "    chart_air_final = chart_air_src.set_index('DATE_OBSERVATION').resample(resample_rule)['VALEUR'].mean().reset_index()\n",
    "else:\n",
    "    chart_air_final = pd.DataFrame()\n",
    "\n",
    "if not chart_weather_src.empty:\n",
    "    chart_weather_final = chart_weather_src.set_index('DATE').resample(resample_rule)[['TEMP', 'WDSP', 'DEWP']].mean().reset_index()\n",
    "else:\n",
    "    chart_weather_final = pd.DataFrame()\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. UI PRINCIPALE\n",
    "# ==============================================================================\n",
    "\n",
    "col1, col2 = st.columns([3, 2])\n",
    "\n",
    "# --- COLONNE 1 : CARTE & GRAPHIQUES COMPARAISON ---\n",
    "with col1:\n",
    "    st.subheader(f\"Carte : {selected_polluant}\")\n",
    "    \n",
    "    m = folium.Map(location=[40.7128, -74.0060], zoom_start=10, tiles=\"CartoDB positron\")\n",
    "\n",
    "    choropleth = folium.Choropleth(\n",
    "        geo_data=gdf_display,\n",
    "        data=gdf_display,\n",
    "        columns=['GEOCODE', 'MEAN_POLLUANT'],\n",
    "        key_on='feature.properties.GEOCODE',\n",
    "        fill_color='YlOrRd',\n",
    "        fill_opacity=0.7,\n",
    "        line_opacity=0.2,\n",
    "        legend_name=f\"Concentration {selected_polluant}\",\n",
    "        highlight=True\n",
    "    )\n",
    "    choropleth.add_to(m)\n",
    "\n",
    "    style_function = lambda x: {'fillColor': '#ffffff', 'color':'#000000', 'fillOpacity': 0.0, 'weight': 0.1}\n",
    "    tooltip_layer = folium.GeoJson(\n",
    "        gdf_display,\n",
    "        style_function=style_function,\n",
    "        tooltip=folium.GeoJsonTooltip(\n",
    "            fields=['GEONAME', 'BOROUGH', 'MEAN_POLLUANT', 'W_TEMP', 'W_WIND', 'NB_STATIONS'],\n",
    "            aliases=['Quartier:', 'Borough:', f'{selected_polluant}:', 'Temp (¬∞C):', 'Vent (km/h):', 'Stations:'],\n",
    "            localize=True\n",
    "        )\n",
    "    ).add_to(m)\n",
    "\n",
    "    st_map = st_folium(m, width=None, height=600)\n",
    "    \n",
    "    # --- LOGIQUE SYNCHRONISATION (Invisible) ---\n",
    "    geo_options_df = geo[['GEOCODE', 'GEONAME']].sort_values('GEONAME')\n",
    "    if st_map and st_map.get('last_object_clicked'):\n",
    "        last_clicked = st_map['last_object_clicked']\n",
    "        if isinstance(last_clicked, dict) and 'properties' in last_clicked:\n",
    "            props = last_clicked['properties']\n",
    "            if props and 'GEOCODE' in props:\n",
    "                clicked_code = str(props['GEOCODE'])\n",
    "                name_match = geo_options_df[geo_options_df['GEOCODE'] == clicked_code]['GEONAME']\n",
    "                if not name_match.empty:\n",
    "                    clicked_name = name_match.values[0]\n",
    "                    if st.session_state.dropdown_selector != clicked_name:\n",
    "                        st.session_state.dropdown_selector = clicked_name\n",
    "                        st.session_state.selected_geocode = clicked_code\n",
    "                        st.rerun()\n",
    "\n",
    "    # --- GRAPHIQUES : COMPARAISONS M√âT√âO (SOUS LA CARTE) ---\n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"üìâ Facteurs M√©t√©orologiques\")\n",
    "    \n",
    "    meteo_config = {\n",
    "        'Temp√©rature': {'col': 'TEMP', 'color': 'orange', 'label': 'Temp (¬∞C)'},\n",
    "        'Vitesse Vent': {'col': 'WDSP', 'color': 'blue', 'label': 'Vent (km/h)'},\n",
    "        'Point de Ros√©e': {'col': 'DEWP', 'color': 'green', 'label': 'Ros√©e (¬∞C)'}\n",
    "    }\n",
    "    \n",
    "    if not selected_meteo_vars:\n",
    "        st.info(\"S√©lectionnez des variables m√©t√©o dans le menu pour voir les comparaisons.\")\n",
    "    else:\n",
    "        for var_name in selected_meteo_vars:\n",
    "            fig = go.Figure()\n",
    "            \n",
    "            # Courbe Air (R√©f√©rence)\n",
    "            if not chart_air_final.empty:\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=chart_air_final['DATE_OBSERVATION'], \n",
    "                    y=chart_air_final['VALEUR'], \n",
    "                    name=selected_polluant, \n",
    "                    mode='lines',\n",
    "                    line=dict(color='red', width=1, dash='solid'),\n",
    "                    opacity=0.5\n",
    "                ))\n",
    "\n",
    "            # Courbe M√©t√©o\n",
    "            if not chart_weather_final.empty and var_name in meteo_config:\n",
    "                conf = meteo_config[var_name]\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=chart_weather_final['DATE'], \n",
    "                    y=chart_weather_final[conf['col']], \n",
    "                    name=conf['label'], \n",
    "                    mode='lines+markers',\n",
    "                    marker=dict(size=4),\n",
    "                    line=dict(color=conf['color'], width=2), \n",
    "                    yaxis='y2'\n",
    "                ))\n",
    "\n",
    "            fig.update_layout(\n",
    "                title=f\"{selected_polluant} vs {var_name}\",\n",
    "                xaxis_title=\"Date\",\n",
    "                yaxis=dict(title=selected_polluant, showgrid=False),\n",
    "                yaxis2=dict(title=var_name, overlaying='y', side='right', showgrid=True),\n",
    "                legend=dict(orientation=\"h\", y=1.1),\n",
    "                height=350, margin=dict(t=40, b=0, l=0, r=0)\n",
    "            )\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "# --- COLONNE 2 : D√âTAILS & GRAPHIQUE PRINCIPAL ---\n",
    "with col2:\n",
    "    st.markdown(\"### üìç D√©tails\")\n",
    "    \n",
    "    all_options = [\"Tous quartiers\"] + geo_options_df['GEONAME'].tolist()\n",
    "    \n",
    "    selected_option = st.selectbox(\n",
    "        \"S√©lectionner une zone\",\n",
    "        options=all_options,\n",
    "        key=\"dropdown_selector\"\n",
    "    )\n",
    "    \n",
    "    if selected_option == \"Tous quartiers\":\n",
    "        st.session_state.selected_geocode = None\n",
    "    else:\n",
    "        code_match = geo_options_df[geo_options_df['GEONAME'] == selected_option]['GEOCODE']\n",
    "        if not code_match.empty:\n",
    "            st.session_state.selected_geocode = str(code_match.values[0])\n",
    "\n",
    "    st.title(current_title)\n",
    "    st.caption(current_caption)\n",
    "\n",
    "    # KPIs\n",
    "    kpi1, kpi2, kpi3 = st.columns(3)\n",
    "    val_p = f\"{avg_polluant:.2f}\" if pd.notnull(avg_polluant) else \"N/A\"\n",
    "    val_t = f\"{avg_temp:.1f} ¬∞C\" if pd.notnull(avg_temp) else \"N/A\"\n",
    "    val_w = f\"{avg_wind:.1f} km/h\" if pd.notnull(avg_wind) else \"N/A\"\n",
    "    \n",
    "    kpi1.metric(f\"Moy. {selected_polluant}\", val_p)\n",
    "    kpi2.metric(\"Temp. Moy\", val_t)\n",
    "    kpi3.metric(\"Vent Moy\", val_w)\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    \n",
    "    # --- GRAPHIQUE : √âVOLUTION POLLUANT SEUL (SOUS LES KPIS) ---\n",
    "    st.subheader(\"üìà √âvolution du Polluant\")\n",
    "    \n",
    "    fig_main = go.Figure()\n",
    "    if not chart_air_final.empty:\n",
    "        fig_main.add_trace(go.Scatter(\n",
    "            x=chart_air_final['DATE_OBSERVATION'], \n",
    "            y=chart_air_final['VALEUR'], \n",
    "            name=selected_polluant, \n",
    "            mode='lines+markers',\n",
    "            marker=dict(size=8),\n",
    "            line=dict(color='red', width=3)\n",
    "        ))\n",
    "        fig_main.update_layout(\n",
    "            title=f\"Tendance : {selected_polluant}\",\n",
    "            xaxis_title=\"Date\", \n",
    "            yaxis=dict(title=\"Concentration\"), \n",
    "            height=400,\n",
    "            margin=dict(t=40, b=0, l=0, r=0)\n",
    "        )\n",
    "        st.plotly_chart(fig_main, use_container_width=True)\n",
    "    else:\n",
    "        st.info(\"Pas de donn√©es suffisantes pour afficher l'√©volution.\")\n",
    "\"\"\"\n",
    "\n",
    "with open(\"app.py\", \"w\", encoding='utf-8') as f:\n",
    "    f.write(dashboard_code)\n",
    "\n",
    "print(\"\\nüöÄ Application g√©n√©r√©e ! Lancez dans le terminal : streamlit run app.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e79f871-2d2a-44a3-8a2f-a90a555d0ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
